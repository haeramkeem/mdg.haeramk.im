<!DOCTYPE html>
<html><head><title>(논문) ZNS, Avoiding the Block Interface Tax for Flash-based SSDs</title><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="(논문) ZNS, Avoiding the Block Interface Tax for Flash-based SSDs"/><meta property="og:description" content="이 글은 USENIX ATC &amp;#039;21 에 소개된 ZNS: Avoiding the Block Interface Tax for Flash-based SSDs 논문을 읽고 정리한 것입니다. 본 글은 #draft 상태입니다. 내용 정리 1. Introduction § 본 논문에서는 아래의 다섯가지 contribution 을 소개한다: 기존의 Block SSD 와 ZNS SSD 간의 성능 비교 ZNS 에 대한 전반적인 리뷰 Host software layer 에 ZNS SSD 적용하는 과정에서 배운 점들 ZNS 를 사용하기 위해 Storage stack 전반을 수정한 내용 여기에는 Linux 커널, F2FS 파일시스템, NVMe 드라이버, Zoned Block Device 서브시스템, fio 벤치마크 툴이 포함된다."/><meta property="og:image" content="https://mdg.haeramk.im/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../../../../../static/icon.png"/><meta name="description" content="이 글은 USENIX ATC &amp;#039;21 에 소개된 ZNS: Avoiding the Block Interface Tax for Flash-based SSDs 논문을 읽고 정리한 것입니다. 본 글은 #draft 상태입니다. 내용 정리 1. Introduction § 본 논문에서는 아래의 다섯가지 contribution 을 소개한다: 기존의 Block SSD 와 ZNS SSD 간의 성능 비교 ZNS 에 대한 전반적인 리뷰 Host software layer 에 ZNS SSD 적용하는 과정에서 배운 점들 ZNS 를 사용하기 위해 Storage stack 전반을 수정한 내용 여기에는 Linux 커널, F2FS 파일시스템, NVMe 드라이버, Zoned Block Device 서브시스템, fio 벤치마크 툴이 포함된다."/><meta name="generator" content="Quartz"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link href="../../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Gowun Batang:wght@400;700&amp;family=Gowun Dodum:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch(`../../../../../static/contentIndex.json`).then(data => data.json())</script></head><body data-slug="gardens/storage/zsm/papers/drafts/(논문)-ZNS,-Avoiding-the-Block-Interface-Tax-for-Flash-based-SSDs"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title "><a href="../../../../..">Madison Digital Garden</a></h1><div class="spacer mobile-only"></div><div class="search "><div id="search-icon"><p>Search</p><div></div><svg tabIndex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="results-container"></div></div></div></div><div class="darkmode "><input class="toggle" id="darkmode-toggle" type="checkbox" tabIndex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabIndex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlnsXlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35;" xmlSpace="preserve"><title>Light mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabIndex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlnsXlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'" xmlSpace="preserve"><title>Dark mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><h1 class="article-title ">(논문) ZNS, Avoiding the Block Interface Tax for Flash-based SSDs</h1><p class="content-meta ">Feb 17, 2025, 32 min read</p><ul class="tags "><li><a href="../../../../../tags/논문" class="internal tag-link">#논문</a></li><li><a href="../../../../../tags/storage" class="internal tag-link">#storage</a></li></ul></div></div><article class="popover-hint"><blockquote class="callout" data-callout="info">
<div class="callout-title">
                  <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><line x1="12" y1="16" x2="12" y2="12"></line><line x1="12" y1="8" x2="12.01" y2="8"></line></svg></div>
                  <div class="callout-title-inner"><p>이 글은 USENIX ATC '21 에 소개된 <a href="https://www.usenix.org/system/files/atc21-bjorling.pdf" class="external">ZNS: Avoiding the Block Interface Tax for Flash-based SSDs</a> 논문을 읽고 정리한 것입니다.</p></div>
                  
                </div>
</blockquote>
<blockquote class="callout is-collapsible is-collapsed" data-callout="failure" data-callout-fold>
<div class="callout-title">
                  <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="18" y1="6" x2="6" y2="18"></line><line x1="6" y1="6" x2="18" y2="18"></line></svg></div>
                  <div class="callout-title-inner"><p>본 글은 #draft 상태입니다. </p></div>
                  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold">
                  <polyline points="6 9 12 15 18 9"></polyline>
                </svg>
                </div>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled/> 내용 정리</li>
</ul>
</blockquote>
<h2 id="1-introduction">1. Introduction<a aria-hidden="true" tabindex="-1" href="#1-introduction" class="internal"> §</a></h2>
<ul>
<li>본 논문에서는 아래의 다섯가지 contribution 을 소개한다:
<ol>
<li>기존의 Block SSD 와 ZNS SSD 간의 성능 비교</li>
<li>ZNS 에 대한 전반적인 리뷰</li>
<li>Host software layer 에 ZNS SSD 적용하는 과정에서 배운 점들</li>
<li>ZNS 를 사용하기 위해 Storage stack 전반을 수정한 내용
<ul>
<li>여기에는 Linux 커널, F2FS 파일시스템, NVMe 드라이버, Zoned Block Device 서브시스템, fio 벤치마크 툴이 포함된다.</li>
</ul>
</li>
<li>RocksDB 에서 ZNS SSD 를 사용하기 위한 새로운 스토리지 백엔드로 ZenFS 구현</li>
</ol>
</li>
</ul>
<h2 id="2-the-zoned-storage-model">2. The Zoned Storage Model<a aria-hidden="true" tabindex="-1" href="#2-the-zoned-storage-model" class="internal"> §</a></h2>
<h3 id="21-the-block-interface-tax-기존-방식의-문제점">2.1. The Block Interface Tax: 기존 방식의 문제점<a aria-hidden="true" tabindex="-1" href="#21-the-block-interface-tax-기존-방식의-문제점" class="internal"> §</a></h3>
<ul>
<li><a href="../../../../../gardens/storage/common/terms/Logical-Block-Addressing,-LBA-(Storage)" class="internal" data-slug="gardens/storage/common/terms/Logical-Block-Addressing,-LBA-(Storage)">Block Interface</a> 는 이전의 HDD 를 위해 고안된 것으로, 고정된 크기의 block 들을 1차원 배열로 묶어 사용/관리하는 방식이다.</li>
<li><a href="../../../../../gardens/storage/ssd/Flash-Memory,-SSD-(Storage)" class="internal" data-slug="gardens/storage/ssd/Flash-Memory,-SSD-(Storage)">SSD</a> 가 등장했을 때에는 이러한 Block interface 와의 backward-compatibility 를 위해 <a href="../../../../../gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)">FTL</a> 과 같은 레이어가 추가되었다.</li>
<li>하지만 SSD 와 HDD 는 작동 방식이 다르기에 FTL 은 여러 부작용 을 낳았다.
<ul>
<li><a href="../../../../../gardens/storage/ssd/terms/Garbage-Collection,-GC-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Garbage-Collection,-GC-(Storage)">GC</a> 오버헤드</li>
<li>GC 에서 데이터를 이동할 때 사용할 임시 저장 공간을 위한 <a href="../../../../../gardens/storage/ssd/terms/Over-Provisioning,-OP-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Over-Provisioning,-OP-(Storage)">OP</a></li>
<li><a href="../../../../../gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)">FTL</a> 을 위한 DRAM</li>
<li>GC 오버헤드를 줄이기 위한 Application level 의 최적화 - 에 따라 증가하는 복잡도</li>
<li>성능을 예측하기 힘듦 (performance unpredictability)</li>
</ul>
</li>
<li>위의 자원들 중 OP 와 DRAM 은 모두 현재의 SSD 가 감수해야 하는 아주 비싼 자원들이다.</li>
<li>Block interface 를 위해 고안된 FTL 은 Host 가 LBA 에만 접근할 수 있도록 하여 Host 의 물리적인 데이터 저장 위치를 관리 권한이 박탈되었다.</li>
</ul>
<h3 id="22-existing-tax-reduction-strategies-문제점을-해결하기-위해-이전에-시도된-것들">2.2. Existing Tax-Reduction Strategies: 문제점을 해결하기 위해 이전에 시도된 것들<a aria-hidden="true" tabindex="-1" href="#22-existing-tax-reduction-strategies-문제점을-해결하기-위해-이전에-시도된-것들" class="internal"> §</a></h3>
<ol>
<li><a href="../../../../../gardens/storage/ssd/terms/Multi-stream-SSD-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Multi-stream-SSD-(Storage)">Multi-stream SSD</a>: GC 를 줄이는데 도움은 되나, 여전히 OP 공간과 FTL 을 위한 DRAM 을 필요로 한다.</li>
<li><a href="../../../../../gardens/storage/ssd/terms/Open-Channel-SSD,-OCSSD-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Open-Channel-SSD,-OCSSD-(Storage)">OCSSD</a>: GC 도 줄이고 OP 공간과 DRAM 의 필요성을 줄여주었지만, 모든 SSD 에 대응할 수 있는 인터페이스가 OS 에 구현되어야 하기 때문에 한계점이 있다.</li>
</ol>
<h3 id="23-tax-free-storage-with-zones-문제를-해결하기-위한-접근">2.3. Tax-free Storage with Zones: 문제를 해결하기 위한 접근<a aria-hidden="true" tabindex="-1" href="#23-tax-free-storage-with-zones-문제를-해결하기-위한-접근" class="internal"> §</a></h3>
<ul>
<li>“Backward-compatibility 는 포기하더라도, SSD 를 위한 새로운 방식의 Interface 를 만들자” 라는 생각으로 Block Interface Tax 문제에 접근한 것이 <a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">Zoned Storage Model</a> 이다.</li>
<li>해당 Model 에 대한 NVMe 의 구현체인 ZNS 는 이러한 기능을 제공해 준다:
<ul>
<li>Zoned Storage Model 와 완벽하게 호환됨</li>
<li>SSD 의 특징을 최대한으로 이끌어내 성능을 높임</li>
<li>(OCSSD와는 다르게) 각 디바이스의 특성과는 무관함</li>
</ul>
</li>
</ul>
<h2 id="3-evolving-towards-zns-zns를-도입해-보자">3. Evolving towards ZNS: ZNS를 도입해 보자<a aria-hidden="true" tabindex="-1" href="#3-evolving-towards-zns-zns를-도입해-보자" class="internal"> §</a></h2>
<ul>
<li>ZNS 는 data placement 및 gc 과정을 host 에게 위임함으로써 device level 의 <a href="../../../../../gardens/storage/ssd/terms/Write-Amplification,-WA-and-Write-Amplication-Factor,-WAF-(Storage)" class="internal" data-slug="gardens/storage/ssd/terms/Write-Amplification,-WA-and-Write-Amplication-Factor,-WAF-(Storage)">WAF</a> 를 없앴고, 따라서 OP 도 적게 사용하며 성능과 ssd 의 수명도 늘렸다</li>
</ul>
<h3 id="31-hardware-impact">3.1. Hardware Impact<a aria-hidden="true" tabindex="-1" href="#31-hardware-impact" class="internal"> §</a></h3>
<h4 id="zone-sizing">Zone Sizing<a aria-hidden="true" tabindex="-1" href="#zone-sizing" class="internal"> §</a></h4>
<ul>
<li>Zone 은 여러 die 의 block 들 (이것을 <em>stripe</em> 라고도 한다) 로 구성된다.</li>
<li>Zone size 가 커지는 것은 더욱 많은 die 의 block 들을 응집할 수 있으므로, (1) die-level fault 에서 데이터를 보호하기 쉽고 (2) 병렬 처리율이 높아진다는 장점이 있다.
<ul>
<li>반대의 극단을 생각해보면 이해가 빠르다: zone 의 block 이 한 die 에 속하게 된다면,</li>
<li>(1) 해당 die 에 문제가 생기면 zone 전체가 날라가게 되고 <sup><a href="#user-content-fn-multiple-die-parity" id="user-content-fnref-multiple-die-parity" data-footnote-ref aria-describedby="footnote-label" class="internal">1</a></sup></li>
<li>(2) 해당 die 에만 command 를 실행할 수 있으므로 병렬처리율도 낮아져 성능도 낮아진다.</li>
</ul>
</li>
<li>반대로 커지는 것에 대한 단점은 zone 의 갯수가 작아지기 때문에 data placement 선택권이 낮아져 ZNS 의 이점을 살리지 못한다.
<ul>
<li>만일 storage 전체를 커버하는 zone 하나만이 존재한다고 생각해 보자.</li>
<li>그럼 모든 종류의 lifecycle 을 가지는 data 들이 하나의 zone 에 포함되게 되고, 결국에는 gc 를 해야 하거나 reset 시 valid data 가 날라가게 된다.</li>
</ul>
</li>
<li>따라서 data placement 선택권을 최대한 주기 위해 zone size 를 최대한 줄이되, die-level fault 에 영향과 병렬 처리율 감소를 감당할 수준이 될 정도까지만 줄였다고 한다.</li>
<li>만일 reliability 를 포기한다면 zone(stripe)-wide parity 를 없애서 zone size 를 더욱 더 줄일 수 있으나 host level 에서 이런 parity 를 구현하거나 작아진 zone size 를 위해 IO queue depth 를 늘리는 것과 같은 희생을 감내해야만 한다.</li>
</ul>
<h4 id="mapping-table">Mapping Table<a aria-hidden="true" tabindex="-1" href="#mapping-table" class="internal"> §</a></h4>
<ul>
<li>기존의 Block interface SSD 에서는 <a href="../../../../../gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)#page-level-mapping" class="internal" data-slug="gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)">Fully-associative mapping table (Page level mapping)</a> 을 사용했는데, 이것은 당연히 엄청난 양의 DRAM 공간을 필요로 한다.</li>
<li>하지만 ZNS 의 Sequential write constraint 는 이것을 <a href="../../../../../gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)#block-level-mapping" class="internal" data-slug="gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)">erase block level</a> 혹은 <a href="../../../../../gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)#hybrid-log-block-ftl" class="internal" data-slug="gardens/storage/ssd/terms/Flash-Translation-Layer,-FTL-(Storage)">hybrid fashion</a> 으로 바꿀 수 있게 해서 DRAM 공간을 줄이거나 필요성을 아예 없앨 수도 있다고 한다.</li>
<li>어떻게 ZNS 가 이런 것을 가능하게 하는지는 설명 안한다.</li>
</ul>
<h4 id="device-resources">Device Resources<a aria-hidden="true" tabindex="-1" href="#device-resources" class="internal"> §</a></h4>
<ul>
<li>이것은 <a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)#open-zone-limit" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">Open Zone Limit</a> 과 연관된 내용이다.</li>
<li>Active zone 의 data 와 parity 를 위해서는 XOR 엔진이나 SRAM, DRAM 같은 자원과 parity 보존을 위한 power capacitor 가 필요하다.</li>
<li>하지만 zone 의 data 와 parity 의 사이즈는 아주 크고 <sup><a href="#user-content-fn-two-step-programming" id="user-content-fnref-two-step-programming" data-footnote-ref aria-describedby="footnote-label" class="internal">2</a></sup> 자원의 양은 한정되어 있기 때문에 8개에서 32개의 active zone 만을 유지할 수 있고 이것이 Open Zone Limit 으로 제한걸려있는 것.</li>
<li>이 active zone 개수를 늘리기 위해서는 다음과 같은 전략을 취할 수 있다고 한다 <sup><a href="#user-content-fn-write-back-cache" id="user-content-fnref-write-back-cache" data-footnote-ref aria-describedby="footnote-label" class="internal">3</a></sup> :
<ul>
<li>더 많은 자원을 구비하거나 (adding extra power capacitors)</li>
<li>최적화를 통해 자원 사용량을 줄이거나 (utilizing DRAM for data movements)</li>
<li>Parity 를 조금 포기하는 방법</li>
</ul>
</li>
</ul>
<h3 id="32-host-software-adoption">3.2. Host Software Adoption<a aria-hidden="true" tabindex="-1" href="#32-host-software-adoption" class="internal"> §</a></h3>
<ul>
<li>이 섹션에서는 ZNS 를 도입하기 위한 host software 변경 세 가지를 소개한다:
<ul>
<li>Host-side FTL (HFTL)</li>
<li>File System</li>
<li>End-to-End Data Placement</li>
</ul>
</li>
<li>ZNS 는 sequential write 가 기본 원리에 깔려있기 때문에, sequential write 를 자주 사용하는 application 에 적합하다.
<ul>
<li>가령 <a href="../../../../../gardens/database/dbms/rocksdb/terms/LSM-Tree-(RocksDB)" class="internal" data-slug="gardens/database/dbms/rocksdb/terms/LSM-Tree-(RocksDB)">LSM Tree</a> 와 같은 것들</li>
<li>당연히 in-place update 가 잦은 시스템은 ZNS 도입이 어렵다.</li>
</ul>
</li>
</ul>
<h4 id="host-side-ftl-hftl">Host-side FTL (HFTL)<a aria-hidden="true" tabindex="-1" href="#host-side-ftl-hftl" class="internal"> §</a></h4>
<ul>
<li>이놈은 ZNS 의 sequential write 와 application 의 random write (+ in-place update) 간의 다리 역할을 해준다.</li>
<li>기존의 FTL (SSD FTL) 과의 차이점은
<ul>
<li>이름이 HFTL 인 만큼 SSD 에 포함되는 것이 아닌 Host OS 에 포함되는 계층이라는 것</li>
<li>SSD FTL 의 역할 중 (1) address translation mapping 과 (2) GC 만 담당하는것 이다.</li>
</ul>
</li>
<li>이놈이 host 에서 작동한다는 것은, 다음과 같은 장단점을 가질 수 있다:
<ul>
<li>일단 장점으로는 host 의 정보를 활용하여 data placement 와 gc 를 수행할 수 있도록 해준다</li>
<li>단점 (주의할 점?) 으로는 host 와 cpu 및 mem 을 공유하기에 너무 많은 cpu 와 mem 을 먹지 않도록 해야 한다고 한다.</li>
</ul>
</li>
<li>이것의 구현체는 다음과 같은 것들이 있다고 한다:
<ul>
<li><a href="https://docs.kernel.org/admin-guide/device-mapper/dm-zoned.html" class="external">dm-zoned</a></li>
<li><a href="https://github.com/westerndigitalcorporation/dm-zap" class="external">dm-zap</a> - 논문 작성 시점에는 이놈만 ZNS 를 지원했다고 한다</li>
<li><a href="https://www.kernel.org/doc/html/v5.4/driver-api/lightnvm-pblk.html" class="external">pblk</a></li>
<li><a href="https://spdk.io/doc/ftl.html" class="external">SPDK FTL</a></li>
</ul>
</li>
</ul>
<h4 id="file-system">File System<a aria-hidden="true" tabindex="-1" href="#file-system" class="internal"> §</a></h4>
<ul>
<li>File system 은 application 들이 “file” 형식으로 storage 에 접근할 수 있도록 해준다.</li>
<li>File system 을 zone 과 통합함으로써 <sup><a href="#user-content-fn-primarily-sequential-workload" id="user-content-fnref-primarily-sequential-workload" data-footnote-ref aria-describedby="footnote-label" class="internal">4</a></sup>, 여러 overhead 들 (data placement overhead 혹은 indirect overhead <sup><a href="#user-content-fn-indirection-overhead" id="user-content-fnref-indirection-overhead" data-footnote-ref aria-describedby="footnote-label" class="internal">5</a></sup> 와 같은 FTL 과 HFTL 이 발생시키는 overhead) 을 없앨 수 있다고 한다.</li>
<li>또한 file 의 데이터 특성 정보들을 data placement 에 활용할 수 있다.
<ul>
<li>적어도 이러한 정보들이 file system 에서 인지할 수 있도록 해준다.</li>
</ul>
</li>
<li>대부분의 file system 들은 in-place write 를 사용하기 때문에 zone 을 사용하기 힘들다.</li>
<li>하지만 <a href="../../../../../gardens/os/fs/draft/Flash-Friendly-File-System,-F2FS-(File-System)" class="internal" data-slug="gardens/os/fs/draft/Flash-Friendly-File-System,-F2FS-(File-System)">f2fs</a>, <a href="https://en.wikipedia.org/wiki/Btrfs" class="external">btrfs</a>, <a href="https://en.wikipedia.org/wiki/ZFS" class="external">zfs</a> 는 sequential write 자주 사용하고, 따러서 zone 또한 지원하도록 업데이트 되었다고 한다.
<ul>
<li>물론 모든 write 가 sequential 한 것은 아니다; superblock 이나 metadata 같은 경우에는  in-place write 를 사용한다.</li>
<li>하지만 이러한 non-sequential write 의 경우에는 strict log-structured write, floating superblock 과 같은 방법을 이용해 해결할 수 있다고 한다.</li>
</ul>
</li>
<li>이러한 file system 에는 HFTL 의 로직들이 많이 포함된다고 한다.
<ul>
<li>Metadata 를 통해 on-disk 로 관리되는 LBA mapping table 이랄지</li>
<li>GC 기능이랄지</li>
</ul>
</li>
<li>하지만 논문 작성 당시 위 file system 들은 ZAC/ZBC 만을 지원했고, 따라서 본 논문에서는 f2fs 에서 ZNS 를 지원하도록 개발했다고 한다.</li>
</ul>
<h4 id="end-to-end-data-placement">End-to-End Data Placement<a aria-hidden="true" tabindex="-1" href="#end-to-end-data-placement" class="internal"> §</a></h4>
<ul>
<li>End-to-End Data Placement 는 application 이 중간 fs 와 같은 layer 를 거치지 않고 직접 device 에 data placement 를 실시하게 하는 것이다.
<ul>
<li>이렇게 함으로써 application 에는 data placement 에 대한 최대의 자율성을 제공해 줄 수 있고</li>
<li>동시에 indirection overhead <sup><a href="#user-content-fn-indirection-overhead" id="user-content-fnref-indirection-overhead-2" data-footnote-ref aria-describedby="footnote-label" class="internal">5</a></sup> 도 줄일 수 있다.</li>
<li>또한 application 자체에서 raw block device 에 접근하는 것이기에 성능이 향상되고 WAF 도 줄어든다.</li>
</ul>
</li>
<li>File system 을 포기하는 것은 당연히 쉬운 일은 아니다; file 형식을 사용할 수 없기 때문에, application 에 직접 zone support 를 추가해야 되고, data inspection, error checking, backup/restore operation 을 위한 tool 또한 제공되어야 한다.</li>
<li>이 방법을 사용하기에 용이한 application 은 당연히 sequential write 를 활용하고 있는 것들이고, 여기에는 대표적으로
<ul>
<li>LSM 을 활용하는 RocksDB</li>
<li>캐싱 스토어인 CacheLib</li>
<li>오브젝트 스토어인 Ceph SeaStore</li>
</ul>
</li>
<li>본 논문에서는 RocksDB 에 ZNS E2E Data Placement 를 지원하기 위한 ZenFS 를 개발하였고, 이것을 f2fs 를 사용했을 때 등과 비교했다고 한다.</li>
</ul>
<h2 id="4-implementation">4. Implementation<a aria-hidden="true" tabindex="-1" href="#4-implementation" class="internal"> §</a></h2>
<ul>
<li>본 논문에서는 ZNS support 를 위해 다음의 4가지 기여를 했다고 한다.
<ul>
<li>Linux kernel 수정</li>
<li>F2fs 수정</li>
<li>Fio 수정</li>
<li>ZenFS (ZNS 용 RocksDB backend store) 개발</li>
</ul>
</li>
</ul>
<h3 id="41-general-linux-support">4.1. General Linux Support<a aria-hidden="true" tabindex="-1" href="#41-general-linux-support" class="internal"> §</a></h3>
<ul>
<li>Linux kernel 의 Zoned Block Device (ZBD) subsystem 은 zoned storage model 을 따르는 device 들에 대한 API 를 제공해 준다.
<ul>
<li>이 API 들에는 list devices, report of zones, zone management (open, reset 등) 이 포함된다.</li>
<li>Kernel-level 및 user-level (<code>ioctl</code>) API 모두 제공한다더라.</li>
</ul>
</li>
<li>이런 API 들을 활용해 fio 와 같은 툴들이 device-speecific 하지 않은 zoned storage model operation 을 이용하게 된다.</li>
<li>본 논문에서는,
<ul>
<li>ZBD subsystem 에서 ZNS SSD 를 등록하거나 나열할 수 있게 하기 위해 NVMe 드라이버를 수정했고</li>
<li>ZNS 를 테스트할 때 활용하기 위해 ZBD subsystem 에 <a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)#zone-size-zone-capacity" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">zone capacity</a> attribute 와 <a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)#active-zones-limit" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">active zones limit</a> 을 추가했다고 한다.</li>
</ul>
</li>
</ul>
<h4 id="zone-capacity">Zone Capacity<a aria-hidden="true" tabindex="-1" href="#zone-capacity" class="internal"> §</a></h4>
<ul>
<li>Linux kernel 은 모든 zone 에 대한 zone descriptor data structure <sup><a href="#user-content-fn-zone-descriptor" id="user-content-fnref-zone-descriptor" data-footnote-ref aria-describedby="footnote-label" class="internal">6</a></sup> 들을 host memory 상에 저장해 두고 사용하고, 문제가 생겼을 경우에는 특정 디스크로부터 다시 가져와서 사용한다.</li>
<li>논문의 저자들은 Zone descriptor data structure 에 zone capacity 라는 attribute 를 추가했다고 한다.</li>
<li>그리고 fio 와 f2fs 에서 해당 attribute 를 사용하기 위한 수정을 했다.</li>
<li>fio 의 경우에는 그냥 zone capacity 내에서만 IO 를 수행하도록 수정하면 되었지만,</li>
<li>f2fs 의 경우에는 수정할 것이 많았다고 한다.</li>
</ul>
<p><img src="../../../../../images/zone_cap.png" width="auto" height="auto"/></p>
<ul>
<li>기존의 f2fs 는 다음과 같은 특징을 가졌다고 한다.
<ul>
<li>f2fs capacity (zone capacity 랑 다른거임) 는 segment (2MiB) 단위로 관리되고, 이 segment 들이 모인 것을 section 라고 하며, 이놈의 크기가 zone size 이다</li>
<li>f2fs 는 section 의 segment 들에 sequential write 를 수행하고,</li>
<li>일부만 write 할 수 있는 segment 따위는 없었다.</li>
</ul>
</li>
<li>여기에 zone capacity 를 f2fs 에서 지원하기 위해 다음과 같은 변경이 이루어졌다
<ul>
<li>기존의 세 segment type (free, open, full) 이외에 두개의 segment type (<em>unusable</em>, <em>partial</em>) 이 추가되었다고 한다.
<ul>
<li><em>unusable</em> 은 zone 의 unwritable part (아마 zone size 에서 zone capacity 를 제하고 남은 공간) 을 나타내는 segment</li>
<li><em>partial</em> 은 unwritable 과 writable 이 섞여있는 segment (zone capacity 가 끝나고 unwritable 이 시작되는 부근의 segment)</li>
<li>즉, zone capacity 크기가 segment 크기의 배수가 아닌 경우에 partial 이 생김</li>
</ul>
</li>
</ul>
</li>
<li><a href="../../../../../gardens/storage/hdd/terms/Shingled-Magnetic-Recording-HDD,-SMR-HDD-(Storage)" class="internal" data-slug="gardens/storage/hdd/terms/Shingled-Magnetic-Recording-HDD,-SMR-HDD-(Storage)">SMR HDD</a> 에는 zone capacity 를 지원하지 않기 때문에, zone capacity 는 zone size 와 동일하게 초기값이 설정된다.</li>
</ul>
<h4 id="limiting-active-zones">Limiting Active Zones<a aria-hidden="true" tabindex="-1" href="#limiting-active-zones" class="internal"> §</a></h4>
<ul>
<li><a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)#active-zones-limit" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">Active Zone Limit</a> 은 Zoned Block Device subsystem 에서 device listing 을 할 때 인식되어 kernel/user-space API 로 노출된다.</li>
<li>이것은 ZNS SSD 만의 특징이기에 SMR HDD 에는 필요하지 않는 값이고, 따라서 0으로 초기화된다.</li>
<li>Fio 의 경우에는 이것을 위한 수정사항은 없다고 한다; 그냥 사용할 때 알아서 이 limit 을 지켜야 하고, 만일 그렇지 않는다면 IO error 가 발생한다.</li>
<li>F2fs 의 경우에는 이 limit 이 f2fs 의 open segment limit <sup><a href="#user-content-fn-open-segment-limit" id="user-content-fnref-open-segment-limit" data-footnote-ref aria-describedby="footnote-label" class="internal">7</a></sup> 과 결부된다. f2fs 에서는 open segment limit 을 6으로 제한해 놓았고, 만일 device 가 active zone limit 을 6까지 지원하지 않는 다면, 그에 맞게 조정할 수 있다.</li>
</ul>
<blockquote class="callout" data-callout="note">
<div class="callout-title">
                  <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="18" y1="2" x2="22" y2="6"></line><path d="M7.5 20.5 19 9l-4-4L3.5 16.5 2 22z"></path></svg></div>
                  <div class="callout-title-inner"><p>주인장의 한마디 </p></div>
                  
                </div>
<ul>
<li>Zone 은 여러 segment 들로 구성되는 것을 고려해 보면, open segment limit 이 너무 적은 것 아니냐 싶을 수 있다.</li>
<li>근데 아마 zone 당 open segment 를 1개씩만 유지하는 것이 아닐까 싶다.</li>
<li>Zone 은 어차피 writing pointer 가 있는 segment 에만 write 할 것이기 때문에, 해당 segment 만 open 하면 되지 않을까.</li>
<li>따라서 f2fs 의 open segment limit 이 6이기 때문에, f2fs 를 사용할 때에는 active zone limit 도 6을 넘지 못할 것 같다.</li>
</ul>
</blockquote>
<ul>
<li>F2fs 는 메타데이터를 <a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)#conventional-zone" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">Conventional Zone</a> 에 저장하는 것을 필요로 한다.
<ul>
<li>Zoned Storage Model 에도 있는 만큼 ZNS SSD 에서 이것을 지원하는 것이 기본이고, 따라서 본 논문의 저자도 이것과 관련해서는 딱히 수정을 하지 않았다고 한다.</li>
<li>하지만 어떤 ZNS SSD 의 경우에는 이것을 지원하지 않는 경우도 있을 것이고, 이때에는 btrfs 나 dm-zap 에서 처럼 <a href="../../../../../gardens/storage/zsm/Zoned-Storage-Model-(Storage)#sequential-write-required-zone" class="internal" data-slug="gardens/storage/zsm/Zoned-Storage-Model-(Storage)">Sequential write required zone</a> 에서 일반적인 block interface 를 제공하도록 기능을 추가할 수도 있을 것이다.</li>
</ul>
</li>
<li>F2fs 에서는 Zoned device 들에서 <a href="../../../../../gardens/os/fs/terms/Slack-Space-Recycling,-SSR-(File-System)" class="internal" data-slug="gardens/os/fs/terms/Slack-Space-Recycling,-SSR-(File-System)">SSR</a> 기능을 사용하지 못하도록 되어 있다.
<ul>
<li>이것은 어느 정도 성능 저하를 유발할 수 있는데, 그럼에도 불구하고 ZNS SSD 의 전반적인 빠른 성능때문에, 일반 SSD 에서 f2fs-SSR 기능을 사용했을 때보다 더 성능이 좋았다고 한다.</li>
</ul>
</li>
</ul>
<h3 id="42-rocksdb-zone-support">4.2. RocksDB Zone Support<a aria-hidden="true" tabindex="-1" href="#42-rocksdb-zone-support" class="internal"> §</a></h3>
<ul>
<li>이 섹션에서는 RocksDB 에서 Zoned device 를 사용하기 위해 ZenFS 라는 backend store 를 구현한 것에 대해 설명한다.
<ul>
<li>ZenFS 는 RocksDB 의 LSM 과 seqential-only compaction 를 ZNS SSD 에서 효율적으로 수행할 수 있게 도와준다.</li>
</ul>
</li>
</ul>
<h4 id="rocksdb-기본-설명">RocksDB 기본 설명<a aria-hidden="true" tabindex="-1" href="#rocksdb-기본-설명" class="internal"> §</a></h4>
<h5 id="log-structured-merge-tree">Log Structured Merge Tree<a aria-hidden="true" tabindex="-1" href="#log-structured-merge-tree" class="internal"> §</a></h5>
<ul>
<li><a href="../../../../../gardens/database/dbms/rocksdb/terms/LSM-Tree-(RocksDB)" class="internal" data-slug="gardens/database/dbms/rocksdb/terms/LSM-Tree-(RocksDB)">RocksDB 에서의 LSM tree</a> 는 인메모리 <a href="../../../../../gardens/database/dbms/rocksdb/terms/Memtable-(RocksDB)" class="internal" data-slug="gardens/database/dbms/rocksdb/terms/Memtable-(RocksDB)">Memtable</a> 와 저장장치에의 여러 level 들 (L0 ~ LMAX) 로 구성되는데</li>
<li>우선 memtable 에 데이터가 저장되고, 이후 주기적 혹은 memtable 의 공간이 부족해지면 L0 로 flush 된다.
<ul>
<li>Memtable 의 내용은 WAL 이 작성되어 crash recovery 를 지원한다.</li>
<li>Memtable 에서 L0 으로 flush 될 때는 key 를 기준으로 중복 제거 및 정렬되어 <a href="../../../../../gardens/database/dbms/rocksdb/terms/Static-Sorted-Table,-SST-(RocksDB)" class="internal" data-slug="gardens/database/dbms/rocksdb/terms/Static-Sorted-Table,-SST-(RocksDB)">SST</a> 로 저장된다.</li>
<li>이 SST 들은 변경이 불가능하고 sequential write 되며 SST 단위로 생성과 삭제된다.</li>
</ul>
</li>
<li>Level 은 사이즈가 exponential 하게 증가하고, 각 level 에는 여러 SST 파일들이 key 범위가 겹치지 않게 저장된다.</li>
<li>하위 level 로 내리는 작업은 <a href="../../../../../gardens/database/dbms/rocksdb/terms/Leveled-Compaction-(RocksDB)" class="internal" data-slug="gardens/database/dbms/rocksdb/terms/Leveled-Compaction-(RocksDB)">Compaction</a> 를 통해 수행된다.
<ul>
<li>Compaction 에서는 현재 level 의 SST 와 다음 level 의 SST 가 병합되어 다름 level 의 새로운 SST 로 생성되게 된다.</li>
<li>낮은 level 로 갈수록 오래된 key-value 가 저장되기에 자연스럽게 hot-cold separation 이 달성된다.</li>
</ul>
</li>
</ul>
<h5 id="backend-storage-api">Backend Storage API<a aria-hidden="true" tabindex="-1" href="#backend-storage-api" class="internal"> §</a></h5>
<ul>
<li>RocksDB 는 File System Wrapper API 를 통해 다양한 backend storage 를 단일 인터페이스를 통해 사용할 수 있도록 설계되어 있다.</li>
<li>Wrapper API 는 데이터의 단위 (SST 파일 혹은 WAL 등) 을 고유한 식별자 (파일 이름 등) 으로 식별한다.</li>
<li>이 식별자들은 파일과 같은 하나의 연속된 주소 공간 (byte-addressable linear address space) 상에 저장된다.</li>
<li>이 식별자들에 대해서는 add, remove, current size, utilization, random access, sequential r/w 의 연산이 가능하다.</li>
<li>따라서 이런 연산들은 file interface 과 유사하기에, RocksDB 도 file 을 주된 backend store 로 사용한다.</li>
<li>즉, 데이터 접근이나 버퍼링, free space management 등을 fs 의 도움을 받아서 사용하는 것이 흔하나</li>
<li>File system 을 거치지 않고 zone 에 직접 data 를 넣는 e2e 방식을 사용하면 더욱 성능이 좋아질 것이기에 ZenFS 를 개발하게 된 것.</li>
</ul>
<h4 id="421-zenfs-architecture">4.2.1. ZenFS Architecture<a aria-hidden="true" tabindex="-1" href="#421-zenfs-architecture" class="internal"> §</a></h4>
<ul>
<li>정리하자면 ZenFS 는 ZNS SSD 의 특성을 최대한 살려 RocksDB 에서 사용할 수 있도록 FS 의 필수적인 것만 구현한 것이다.</li>
</ul>
<h5 id="journaling-and-data">Journaling and Data<a aria-hidden="true" tabindex="-1" href="#journaling-and-data" class="internal"> §</a></h5>
<ul>
<li>ZenFS 는 두가지 종류의 zone 이 존재한다:
<ul>
<li><em>Journaling Zone</em>: 이것은 다음과 같은 용도로 사용된다:
<ol>
<li>FS 의 crash recovery</li>
<li>Superblock 구조 유지</li>
<li>WAL 과 data file 을 zone 에 매핑</li>
</ol>
</li>
<li><em>Data Zone</em>: 일반적인 data file 을 저장</li>
</ul>
</li>
</ul>
<h5 id="extents">Extents<a aria-hidden="true" tabindex="-1" href="#extents" class="internal"> §</a></h5>
<ul>
<li>Data file 들은 <em>Extent</em> 라는 단위로 저장되고, 또 이들은 하나의 data zone 에 sequential write 된다.
<ul>
<li><em>Extent</em> 는 (1) 가변 크기 (2) 블럭 단위 (3) 연속된 공간 이라는 특징을 가진다.</li>
<li>특정한 ID 와 연관된 데이터들이 들어있다</li>
<li>하나의 zone 에는 여러 extent 가 들어가지만 extent 가 zone 을 넘칠 수는 없다.</li>
</ul>
</li>
<li>Extent 에 대한 metadata <sup><a href="#user-content-fn-extent-metadata" id="user-content-fnref-extent-metadata" data-footnote-ref aria-describedby="footnote-label" class="internal">8</a></sup> 는 인메모리에 자료구조에 기록된다.
<ul>
<li>Extent metadata 에는 를 할당하고 반환하는 것, extent 가 어느 zone 에 매핑되었는지 추적하는 등이 포함된다.</li>
<li>이것은 Extent 의 file 이 close 되거나 rocksdb 가 명시적으로 flush 하면 journal zone 에 기록된다.</li>
<li>Zone 의 모든 extent 에 할당된 파일들이 모두 삭제되면 zone 은 reset 될 수 있다.</li>
</ul>
</li>
</ul>
<h5 id="superblock">Superblock<a aria-hidden="true" tabindex="-1" href="#superblock" class="internal"> §</a></h5>
<ul>
<li>Superblock 은 ZenFS 를 초기화하거나 복구할 때 첫 진입점이 된다.</li>
<li>현재의 상태에 대한 ID 값, magic value <sup><a href="#user-content-fn-magic-value" id="user-content-fnref-magic-value" data-footnote-ref aria-describedby="footnote-label" class="internal">9</a></sup>, user option 등이 저장된다.</li>
<li>“현재의 상태에 대한 ID 값” 은 system 에서의 device 순서가 바뀌어도 (가령 재부팅 이후 <code>/dev/nvme0</code> 에서 <code>/dev/nvme1</code> 로 바뀌는 등) filesystem 을 식별할 수 있게 해준다.</li>
</ul>
<h5 id="journal">Journal<a aria-hidden="true" tabindex="-1" href="#journal" class="internal"> §</a></h5>
<ul>
<li>ZenFS 가 journaling 을 하는 것은 아래의 두가지 목적을 위해서 이다:
<ul>
<li>Superblock 저장</li>
<li>WAL 과 data file 이 어떤 extent 를 통해 어떤 zone 에 매핑되었는지를 추적</li>
</ul>
</li>
<li>당연히 journal state 는 journal zone 에 저장되는데, 이것은 <code>OFFLINE</code> 상태가 아닌 첫 두 zone 으로 지정된다.
<ul>
<li>두 zone 은 번갈아 사용되며 journal state update 가 logging 된다.</li>
<li>(F2FS 에서처럼 이전 버전의 journal zone 을 보존해놓는 용도 아닐까)</li>
</ul>
</li>
<li>Journal zone 에는 제일 먼저 header 가 저장된다.
<ul>
<li>Header 에는 (1) sequence number (새로운 journal zone 이 초기화될 때마다 증가하는 값) (2) superblock 자료 구조 (3) 현재의 journal state 에 대한 스냅샷 (journal snapshot) 이 들어간다.</li>
<li>header 가 저장된 이후에는 journal update 들이 쭉 logging 된다.</li>
</ul>
</li>
<li>Journal zone 를 이용해 ZenFS 를 초기화하는 것은 다음과 같은 세 단계로 진행된다.
<ol>
<li>두 journal zone 의 첫 LBA 를 읽은 후, 여기의 header 에서 sequence number 를 읽어와 어떤 journal zone 이 더 최신인지 (어떤 zone 이 active zone 인지, 더 sequence number 가 높은지) 확인한다.</li>
<li>Active zone 의 header 전체를 읽어들여 superblock 과 journal state 를 초기화한다.</li>
<li>journal update 를 쭉 따라가며 journal snapshot 에 반영한다.</li>
</ol>
</li>
<li>Journal update 를 어디까지 따라가며 journal snapshot 에 반영할지는 zone 의 상태와 write pointer 에 따라 결정된다.
<ul>
<li>만일 zone 이 <code>OPEN</code> 혹은 <code>CLOSED</code> 상태라면 현재 write pointer 가 있는 위치까지의 journal update 가 반영된다.</li>
<li>만일 zone 이 <code>FULL</code> 상태라면 header 뒤의 모든 journal update 가 반영된다.
<ul>
<li>만일 zone 이 <code>FULL</code> 상태라면 recovery 이후 나머지 한 journal zone 이 active 가 되며 초기화 후 journal update 를 logging 하게 된다.</li>
</ul>
</li>
</ul>
</li>
<li>Fresh journal state 를 생성하는 것은 타 file system 의 초기화 툴과 비슷한 방식으로 진행된다.
<ul>
<li>ZenFS 의 fresh journal state 초기화 툴은 journal zone 에 sequence number 초기값, superblock, 빈 journal snapshot 로 이루어진 header 를 저장한다.</li>
</ul>
</li>
<li>만일 RocksDB 에 의해 ZenFS 가 초기화되면 상기한 모든 recovery process 가 실행되고, RocksDB 로 부터 데이터를 받아들일 준비가 된다.</li>
</ul>
<h5 id="writable-capacity-in-data-zones">Writable capacity in Data Zones<a aria-hidden="true" tabindex="-1" href="#writable-capacity-in-data-zones" class="internal"> §</a></h5>
<ul>
<li></li>
</ul>
<h5 id="data-zone-selection">Data Zone Selection<a aria-hidden="true" tabindex="-1" href="#data-zone-selection" class="internal"> §</a></h5>
<h5 id="active-zone-limits">Active Zone Limits<a aria-hidden="true" tabindex="-1" href="#active-zone-limits" class="internal"> §</a></h5>
<h5 id="direct-io-and-buffered-writes">Direct I/O and Buffered Writes<a aria-hidden="true" tabindex="-1" href="#direct-io-and-buffered-writes" class="internal"> §</a></h5>
<h2 id="evaluation">Evaluation<a aria-hidden="true" tabindex="-1" href="#evaluation" class="internal"> §</a></h2>
<h3 id="51-raw-io-characteristics">5.1. Raw I/O Characteristics<a aria-hidden="true" tabindex="-1" href="#51-raw-io-characteristics" class="internal"> §</a></h3>
<h3 id="52-rocksdb">5.2. RocksDB<a aria-hidden="true" tabindex="-1" href="#52-rocksdb" class="internal"> §</a></h3>
<h3 id="53-streams">5.3. Streams<a aria-hidden="true" tabindex="-1" href="#53-streams" class="internal"> §</a></h3>
<h2 id="6-related-work">6. Related Work<a aria-hidden="true" tabindex="-1" href="#6-related-work" class="internal"> §</a></h2>
<h2 id="7-conclusion">7. Conclusion<a aria-hidden="true" tabindex="-1" href="#7-conclusion" class="internal"> §</a></h2>
<hr/>
<hr/>
<hr/>
<h2 id="zns-특징">ZNS 특징<a aria-hidden="true" tabindex="-1" href="#zns-특징" class="internal"> §</a></h2>
<ul>
<li>ZNS 는 flash erase boundary 와 write ordering rule 들을 Host 에 노출시켜서 이러한 세금을 해결한다고 한다.</li>
<li>NVMe Zoned Namespace Command Set Specification</li>
<li>기존의 Block interface 는 Block 을 1차원 배열로 취급해 관리했지만, ZNS 는 logical block 들을 zone 으로 묶는다</li>
<li>zone 의 블럭들은 random read 는 가능하지만 무조건 sequential write 되어야 한다.</li>
<li>또한 zone 은 매 rewrite 작업때마다 erase 되어야 한다.</li>
<li>ZNS SSD 는 zone 과  physical media boundary 를 정렬해서 데이터 관리를 Host 가 직접 하도록 한다?</li>
<li>ZNS 는 디바이스들마다 다른 reliability 특성들과 media 관리 복잡성은 Host 로부터 감춘다?</li>
<li>ZNS 는 효율적으로 erase block 을 하기 위한 책임을 Host 로 이전한다.</li>
<li>LBA-PBA 매핑과 데이터 저장 위치 선정을 통합하는 것보다는 이러한 FTL 의 책임 중 일부를 Host 에 이관하는 것은 덜 효과적이다?</li>
</ul>
<h2 id="zns-성능-측정">ZNS 성능 측정<a aria-hidden="true" tabindex="-1" href="#zns-성능-측정" class="internal"> §</a></h2>
<ul>
<li>Concurrent write 수행시 ZNS SSD 가 Block SSD 보다 2.7배 <a href="../../../../../gardens/storage/common/terms/Throughput-(Storage)" class="internal" data-slug="gardens/storage/common/terms/Throughput-(Storage)">Throughput</a> 이 좋았다.</li>
<li>Random read 수행시 ZNS SSD 가 Block SSD 보다 64% 낮은 <a href="../../../../../gardens/storage/common/terms/Latency-(Storage)" class="internal" data-slug="gardens/storage/common/terms/Latency-(Storage)">Latency</a> 를 보여줬다.</li>
<li>RocksDB 를 f2fs + ZNS SSD 와 POSIX fs + Block SSD 에서 사용했을 때, ZNS 를 사용할 때가 2-4 배 낮은 random read latency 를 보여줬다.</li>
<li>또한 RocksDB 를 ZenFS + ZNS 에서 사용했을 때, POSIX + Block SSD 를 사용했을때 보다 2배 높은 throughput 을 보여줬다.</li>
</ul>
<p><img src="../../../../../images/Pasted-image-20240316093233.png" width="auto" height="auto"/></p>
<blockquote>
<p><a href="https://www.usenix.org/system/files/atc21-bjorling.pdf" class="external">출처</a></p>
</blockquote>
<ul>
<li>위 그림은 ZNS SSD 와 OP 가 다르게 설정된 Block SSD 두개 총 세개의 SSD 의 write throughput 을 실험한 것이다.</li>
<li>실험은 SSD 의 물리 사이즈인 2TB 데이터를 총 4번 concurrent 하게 write 하는 방식으로 진행했다. (0<del>2: 첫번째 write, 2</del>4: 두번째 write, 이런식으로 아마?)</li>
<li>첫 write 에서는 SSD 가 fresh 한 상태였기에 세 SSD 간의 성능 차이가 없었지만, 두번째 write 부터는 기존 데이터가 overwrite 되며 Block SSD 들에서 성능 저하가 나타났다.</li>
<li>또한 Block SSD 끼리 비교했을 때에는, OP 를 더 많이 설정한 SSD 가 그나마 성능 저하가 덜 나타나는 것으로 확인됐다.</li>
<li>다만 28% OP 를 먹인 SSD 의 경우에는 공간이 부족하기 때문에 write 가 6TB 을 넘어가자 더이상 기록되지 않는 것을 볼 수 있다.</li>
</ul>
<hr/>
<section data-footnotes class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes<a aria-hidden="true" tabindex="-1" href="#footnote-label" class="internal"> §</a></h2>
<ol>
<li id="user-content-fn-multiple-die-parity">
<p>다만, (1) 의 경우에는 본문에서는 좀 다른 뉘앙스로 말한다. 여러개의 die 에 분산되어 있으면 parity 를 구성하기 용이하기에 여러 die 에 분산시키는 것이 protection 에 유리하다고 한다. - <em>“There is a direct correlation between a zone’s write capacity and the size of the erase block implemented by the SSD. In a block-interface SSD, the erase block size is selected such that data is striped across multiple flash dies, both to gain higher read/write performance, but also to protect against die-level and other media failures through per-stripe parity.”</em> <a href="#user-content-fnref-multiple-die-parity" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-two-step-programming">
<p>본문에서는 zone 사이즈가 큰 이유로 <a href="../../../../../gardens/storage/ssd/draft/One-shot-Programming-(Storage)" class="internal" data-slug="gardens/storage/ssd/draft/One-shot-Programming-(Storage)">Two-step programming</a> 을 언급하지만, 어떤 연관이 있는지 모르겠다. <a href="#user-content-fnref-two-step-programming" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-write-back-cache">
<p>본문에서는 SLC 에서의 Write-back cache 를 사용하는 방법도 소개한다. 이게 뭔지는 모르겡누 <a href="#user-content-fnref-write-back-cache" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-primarily-sequential-workload">
<p>본문에서는 통합의 예시로 “ensuring a primarily sequential workload” 를 제시하는데 감이 안온다. <a href="#user-content-fnref-primarily-sequential-workload" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-indirection-overhead">
<p>본문에서 말하는 <em>“Indirection overhead”</em> 는 <a href="https://www.usenix.org/system/files/conference/inflow14/inflow14-yang.pdf" class="external">Don’t stack your Log on my Log, INFLOW ‘14</a> 논문에서 제기된 문제를 말하는 것이다. 해당 논문에서는 sequential write 를 활용하기 위한 방법 중 하나인 log-structured 를 중복해서 사용하는 것 (가령, log-structured fs 위에서 log-structured application 을 사용하는 등) 이 오히려 성능 저하를 유발한다고 한다. <a href="#user-content-fnref-indirection-overhead" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a> <a href="#user-content-fnref-indirection-overhead-2" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩<sup>2</sup></a></p>
</li>
<li id="user-content-fn-zone-descriptor">
<p>“Zone Descriptor Data Structure” 는 본문에 별도의 설명이 되어 있지는 않지만, 일단 zone 의 메타데이터로 생각하고 넘어가자. 아마 “Zone Descriptor” 는 zone 의 attribute 들에 대한 key-value 쌍이 아닐까. <a href="#user-content-fnref-zone-descriptor" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-open-segment-limit">
<p>본문에는 “Open Segment Limit” 과 같은 용어는 나오지 않는다. 대신 “The number of segments that can be open simultaneously” 라는 표현이 나오며, 주인장이 이것을 단어화 한 것. <a href="#user-content-fnref-open-segment-limit" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-extent-metadata">
<p>“Extent metadata” 라는 용어는 없다. 이해를 위해 주인장이 임의로 쓴 용어이다. <a href="#user-content-fnref-extent-metadata" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-magic-value">
<p>이게 무엇인지는 자세한 설명이 안나온다. <a href="#user-content-fnref-magic-value" data-footnote-backref class="data-footnote-backref internal" aria-label="Back to content">↩</a></p>
</li>
</ol>
</section></article></div><div class="right sidebar"><div class="graph "><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[]}"></div><svg version="1.1" id="global-graph-icon" xmlns="http://www.w3.org/2000/svg" xmlnsXlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xmlSpace="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
	s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
	c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
	C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
	c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
	v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
	s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
	C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
	S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
	s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
	s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[]}"></div></div></div><div class="toc desktop-only"><button type="button" id="toc"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#1-introduction" data-for="1-introduction">1. Introduction</a></li><li class="depth-0"><a href="#2-the-zoned-storage-model" data-for="2-the-zoned-storage-model">2. The Zoned Storage Model</a></li><li class="depth-1"><a href="#21-the-block-interface-tax-기존-방식의-문제점" data-for="21-the-block-interface-tax-기존-방식의-문제점">2.1. The Block Interface Tax: 기존 방식의 문제점</a></li><li class="depth-1"><a href="#22-existing-tax-reduction-strategies-문제점을-해결하기-위해-이전에-시도된-것들" data-for="22-existing-tax-reduction-strategies-문제점을-해결하기-위해-이전에-시도된-것들">2.2. Existing Tax-Reduction Strategies: 문제점을 해결하기 위해 이전에 시도된 것들</a></li><li class="depth-1"><a href="#23-tax-free-storage-with-zones-문제를-해결하기-위한-접근" data-for="23-tax-free-storage-with-zones-문제를-해결하기-위한-접근">2.3. Tax-free Storage with Zones: 문제를 해결하기 위한 접근</a></li><li class="depth-0"><a href="#3-evolving-towards-zns-zns를-도입해-보자" data-for="3-evolving-towards-zns-zns를-도입해-보자">3. Evolving towards ZNS: ZNS를 도입해 보자</a></li><li class="depth-1"><a href="#31-hardware-impact" data-for="31-hardware-impact">3.1. Hardware Impact</a></li><li class="depth-1"><a href="#32-host-software-adoption" data-for="32-host-software-adoption">3.2. Host Software Adoption</a></li><li class="depth-0"><a href="#4-implementation" data-for="4-implementation">4. Implementation</a></li><li class="depth-1"><a href="#41-general-linux-support" data-for="41-general-linux-support">4.1. General Linux Support</a></li><li class="depth-1"><a href="#42-rocksdb-zone-support" data-for="42-rocksdb-zone-support">4.2. RocksDB Zone Support</a></li><li class="depth-0"><a href="#evaluation" data-for="evaluation">Evaluation</a></li><li class="depth-1"><a href="#51-raw-io-characteristics" data-for="51-raw-io-characteristics">5.1. Raw I/O Characteristics</a></li><li class="depth-1"><a href="#52-rocksdb" data-for="52-rocksdb">5.2. RocksDB</a></li><li class="depth-1"><a href="#53-streams" data-for="53-streams">5.3. Streams</a></li><li class="depth-0"><a href="#6-related-work" data-for="6-related-work">6. Related Work</a></li><li class="depth-0"><a href="#7-conclusion" data-for="7-conclusion">7. Conclusion</a></li><li class="depth-0"><a href="#zns-특징" data-for="zns-특징">ZNS 특징</a></li><li class="depth-0"><a href="#zns-성능-측정" data-for="zns-성능-측정">ZNS 성능 측정</a></li></ul></div></div><div class="backlinks "><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer class><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.1.0</a>, © 2025</p><ul><li><a href="https://github.com/haeramkeem">GitHub</a></li><li><a href="https://www.linkedin.com/in/haeram-kim-277404220">LinkedIn</a></li><li><a href="mailto:haeram.kim1@gmail.com">Email</a></li></ul></footer></div></body><script type="application/javascript">// quartz/components/scripts/quartz/components/scripts/callout.inline.ts
function toggleCallout() {
  const outerBlock = this.parentElement;
  outerBlock.classList.toggle(`is-collapsed`);
  const collapsed = outerBlock.classList.contains(`is-collapsed`);
  const height = collapsed ? this.scrollHeight : outerBlock.scrollHeight;
  outerBlock.style.maxHeight = height + `px`;
  let current = outerBlock;
  let parent = outerBlock.parentElement;
  while (parent) {
    if (!parent.classList.contains(`callout`)) {
      return;
    }
    const collapsed2 = parent.classList.contains(`is-collapsed`);
    const height2 = collapsed2 ? parent.scrollHeight : parent.scrollHeight + current.scrollHeight;
    parent.style.maxHeight = height2 + `px`;
    current = parent;
    parent = parent.parentElement;
  }
}
function setupCallout() {
  const collapsible = document.getElementsByClassName(
    `callout is-collapsible`
  );
  for (const div of collapsible) {
    const title = div.firstElementChild;
    if (title) {
      title.removeEventListener(`click`, toggleCallout);
      title.addEventListener(`click`, toggleCallout);
      const collapsed = div.classList.contains(`is-collapsed`);
      const height = collapsed ? title.scrollHeight : div.scrollHeight;
      div.style.maxHeight = height + `px`;
    }
  }
}
document.addEventListener(`nav`, setupCallout);
window.addEventListener(`resize`, setupCallout);
</script><script type="module">
          import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
          const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
          mermaid.initialize({
            startOnLoad: false,
            securityLevel: 'loose',
            theme: darkMode ? 'dark' : 'default'
          });
          document.addEventListener('nav', async () => {
            await mermaid.run({
              querySelector: '.mermaid'
            })
          });
          </script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="https://www.googletagmanager.com/gtag/js?id=G-N68CCP1QHG" type="application/javascript"></script><script src="../../../../../postscript.js" type="module"></script></html>