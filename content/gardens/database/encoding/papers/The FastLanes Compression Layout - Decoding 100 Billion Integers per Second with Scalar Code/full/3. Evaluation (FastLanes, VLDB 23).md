---
tags:
  - database
  - db-encoding
date: 2024-09-04
title: (논문) The FastLanes Compression Layout - Decoding 100 Billion Integers per Second with Scalar Code (3. Evaluation)
---
> [!info] 본 글은 논문 [The FastLanes Compression Layout: Decoding > 100 Billion Integers per Second with Scalar Code (VLDB '23)](https://dl.acm.org/doi/10.14778/3598581.3598587) 를 읽고 정리한 글입니다.

> [!info] 별도의 명시가 없는 한, 본 글의 모든 그림은 위 논문에서 가져왔습니다.

> [!info]- 목차
> - [[1. Introduction (FastLanes, VLDB 23)|1. Introduction]]
> - [[2. FastLanes (FastLanes, VLDB 23)|2. FastLanes]]
> - [[3. Evaluation (FastLanes, VLDB 23)|3. Evaluation (현재 글)]]
> - [[4. Related Work (FastLanes, VLDB 23)|4. Related Work]]
> - [[5. Conclusion and Future Work (FastLanes, VLDB 23)|5. Conclusion and Future Work]]

## 3. Evaluation

### 3.0. Overview

- *FastLanes* 는 C++ 라이브러리로 제작되어, MIT license 로 오픈소스화 되었다. ([링크](https://github.com/cwida/FastLanes))
- Evaluation 을 통해 저자가 답하려고 하는 질문들은 다음과 같다:
	1. [[2. FastLanes (FastLanes, VLDB 23)#2.1.3. Interleaving Bit-packing|Interleaving BP]] 에서의 [[2. FastLanes (FastLanes, VLDB 23)#2.2.4. Usage example Unpacking interleaving-BP|unpacking]] 의 실제 속도는 어느정도일까?
	2. Decoding performance 가 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] reg bit-width 에 따라 실제로 얼마나 달라질까? 그리고 같은 reg bit-width 에 대해, 다양한 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] [[Instruction Set Architecture, ISA (Arch)|ISA]] 간의 성능은 얼마나 차이날까?
	3. [[2. FastLanes (FastLanes, VLDB 23)#2.1.3. Interleaving Bit-packing|Interleaving BP]] 와 [[2. FastLanes (FastLanes, VLDB 23)#2.4. The Unified Transposed Layout|UTL]] 이 scalar code 에 도움을 줄까? [^scalar-code-profit]
	4. 순수한 scalar code 의 performance 는 어느정도일까? 그리고 compiler 에 의한 auto-vectorization 은 explicit [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] [[Intrinsic Function (Arch)|intrinsic]] version 과 비교할 때 어느 정도의 성능일까?
	5. [[2. FastLanes (FastLanes, VLDB 23)#2.4. The Unified Transposed Layout|UTL]] 의 decoding performance 은 어느정도일까?
	6. *FastLanes* 를 도입했을 때의 query execution 은 얼마나 향상될까? 즉, 모든 것이 종합된 E2E 성능 향상은 어느정도일까?
- 또한 [[2. FastLanes (FastLanes, VLDB 23)#2.2.4. Usage example Unpacking interleaving-BP|unpacking]] 와 decoding kernel 을 fusing [^fusing-unpacking] 했을 때의 성능 향상은 어느정도일지도 실험했다고 한다.
- 선행연구과의 비교도 당연히 수행했는데, 이건 [[4. Related Work (FastLanes, VLDB 23)|section 4]] 에서 설명한다고 한다.

### 3.1. Micro-benchmarks

### 3.1.0. Basic setup

- 제안한 [[2. FastLanes (FastLanes, VLDB 23)#2.1.3. Interleaving Bit-packing|Interleaving BP]] 및 여러 decoding 방법은 모든 lane bit-width 경우의 수 ($T \in \{8, 16, 32, 64\}$) 에 대해 구현이 되었고,
- 또한 이들을 다음의 4가지 버전에 대해서도 모두 구현했다고 한다. 진짜 미친 노가다네
	1. `Scalar`: 순수한 scalar code 이다. 어떠한 vectorization 도 들어가지 않은
		- 즉, compiler 에서 vectorization 관련 option 이 꺼져있는 상태다.
	2. `Scalar_T64`: 이건 vectorization 은 하지 않았지만, `uint64` 자료형을 $S=64/T$ 의 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] 로 가정하는 구현체이다.
		- 즉, 그렇게 "가정" 하는 것이고 machine code 관점에서 보아도 vectorization 은 적용되어있지 않은 것.
		- "가상의" software-implemented [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] 라고 이해하면 될듯
	3. `SIMD`: 이건 명시적으로 [[Instruction Set Architecture, ISA (Arch)|ISA]]-specific [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] [[Intrinsic Function (Arch)|intrinsic]] 을 사용한 버전이다.
	4. `Auto-vectorized`: 이건 암묵적인 `SIMD` 라고 생각하면 된다. 즉, compiler 에 의해 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] [[Instruction Set Architecture, ISA (Arch)|ISA]] 로 변환되는 버전이다.
		- 이건 `Scalar` 에서 vectorization compiler option 만 켜서 컴파일 한 것이다.
- 정리하면 다음과 같다고 할 수 있다.

| NAME              | VECTORIZATION AWARE | SIMD INSTRUCTION | SIMD INTRINSIC |
| ----------------- | ------------------- | ---------------- | -------------- |
| `Scalar`          | X                   | X                | X              |
| `Scalar_T64`      | O                   | X                | X              |
| `SIMD`            | O                   | O                | O              |
| `Auto-vectorized` | O                   | O                | X              |

- Compiler 는 LLVM (`clang++`) 를 사용했고, `Scalar` (및 `Scalar_T64`) 에서는 다음의 옵션으로 vectorization 을 비활성화했다고 한다:

```bash
-O3 -mno-sse -fno-slp-vectorize -fno-vectorize
```

- 비교한 CPU 정보는 다음과 같다:

![[Pasted image 20240905164120.png]]

- 여기서 ARM64 에 대해 NEON 만을 사용한 것은 AWS Graviton 에서 실험했을 때 SVE 가 NEON 보다 더 느렸기 때문이라고 한다.
- 이 Micro-benchmark 에서는 순수한 CPU cost 를 확인하고자 했었고 [^pure-cpu-cost], 하나의 vector 를 3000만번 (30M) decoding 하였다고 한다.
	- 그래서 데이터들이 L1 cache 에서만 놀 수 있게 하였다고 한다 [^l1-cache-resident].
- Value 하나 당 소모하는 CPU cycle (즉, 낮을수록 좋다) 을 측정했다고 한다.
	- 다만, $T=8$ 에서의 [[2. FastLanes (FastLanes, VLDB 23)#2.2.4. Usage example Unpacking interleaving-BP|unpacking]] 과정만 반대로 CPU cycle 당 value 를 측정했는데,
	- 이건 해당 지표가 CPU platform 간의 차이를 더 명확하게 보여주기 때문이라고 한다.
	- 즉, 소모 시간 (= CPU cycle) 로 platform 간 비교를 하기에는 각 CPU 들의 타겟층 (PC vs server) 와 CPU freq. 등이 차이가 있기 때문이다.
- 마지막으로, 모든 CPU 는 "Turbo scaling feature" 를 비활성화해 CPU freq. 를 일정하게 유지되게 했다고 한다 [^cpu-turbo-scaling-feat].

[^scalar-code-profit]: 구체적으로 뭔소리인지는 좀 더 읽어보고 판단해야겠다.
[^fusing-unpacking]: 이것도 뭔소린지는 아직까지는 모르겠다.
[^pure-cpu-cost]: 뭔소리임
[^l1-cache-resident]: 3000만번 실험을 돌린거랑 L1 cache resident 가 뭔상관인지 모르겠다.
[^cpu-turbo-scaling-feat]: 내용을 이해하기에 critical 한 것은 아니지만, 이게 뭔지, 그리고 원문에의 "CPU clock normalization stable" 이 뭔소린지 잘 모르겠다.