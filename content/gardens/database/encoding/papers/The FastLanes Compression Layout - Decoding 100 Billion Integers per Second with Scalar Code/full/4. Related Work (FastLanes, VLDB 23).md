---
tags:
  - database
  - db-encoding
date: 2024-09-04
title: (논문) The FastLanes Compression Layout - Decoding 100 Billion Integers per Second with Scalar Code (4. Related Work)
---
> [!info] 본 글은 논문 [The FastLanes Compression Layout: Decoding > 100 Billion Integers per Second with Scalar Code (VLDB '23)](https://dl.acm.org/doi/10.14778/3598581.3598587) 를 읽고 정리한 글입니다.

> [!info] 별도의 명시가 없는 한, 본 글의 모든 그림은 위 논문에서 가져왔습니다.

> [!info]- 목차
> - [[1. Introduction (FastLanes, VLDB 23)|1. Introduction]]
> - [[2. FastLanes (FastLanes, VLDB 23)|2. FastLanes]]
> - [[3. Evaluation (FastLanes, VLDB 23)|3. Evaluation]]
> - [[4. Related Work (FastLanes, VLDB 23)|4. Related Work (현재 글)]]
> - [[5. Conclusion and Future Work (FastLanes, VLDB 23)|5. Conclusion and Future Work]]

## 4. Related Work

- 지난 20년 가까이 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] 를 이용해 compression, decompression 속도를 늘려 [[Database Management System, DBMS (Database)|DBMS]] 의 성능을 향상시키려는 노력들이 수행되었다.
- 이 section 에서는 그런 노력들이 어떤 것이 있는지, 그리고 그것들에 비해 *FastLanes* 가 얼마나 좋은지 알아보자.

### 4.1. Bit-packing

- 일단 전통적인 [[Bit Packing, BP (Encoding)|BP]] 의 변천사는 다음처럼 정리해 볼 수 있다.
	- [[Patching (Encoding)|PA봉크 본인의 옛날 논문]] 에서는 patching 을 이용해 일정한 bit-width 로 [[Bit Packing, BP (Encoding)|BP]] 하는 방법을 제시했고
	- [이 논문(29)](https://dl.acm.org/doi/10.1145/1869389.1869394) 에서는 그것을 *Horizontal* 이라고 불렀으며
	- [이 논문(35)](https://dl.acm.org/doi/10.14778/1687627.1687671) 에서는 그것을 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]]-friendly 하게 바꾸었다.
- 근데 [이 논문(29)](https://dl.acm.org/doi/10.1145/1869389.1869394) 에서는 추가적으로 *k-way* 라는 방법을 새로 제시하였는데,
	- 이건 [[Bit Packing, BP (Encoding)|BP]] 된 value 들을 $k$ -bit 씩 잘라 연속된 memory 공간에 배치하는 방법이다.
	- 이 방식은 *k-way* 말고도 *Vertical layout* 혹은 *Interleaving* (!!) 이라고도 불린다.
- *FastLanes* 에서는 이 아이디어를 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] 에 적용하고 이름을 빌려와 *Interleaving BP* 를 제안하고 있는 것.
- 물론 이와 유사한 것이 이전에도 나오긴 했다.
	- [[2. Background (BtrBlocks, SIGMOD 23)#2.2.5. FOR & Bit-packing|BtrBlocks 에서도 언급한]] Lemire 아자씨의 SIMD-FastBP128 은 SSE SIMD 에 4-way vertical layout 을 적용하였고,
	- 그 이후 AVX2, AVX512 가 등장하며 8, 16-way 도 등장했다.
- 하지만 이들과 *FastLanes* 에서의 *Interleaving BP* 와의 차이점은 이전 애들은 특정 SIMD reg 크기에 한정되어있는 방법이라는 것이다.
	- 따라서 당연히 다른 BP bit-width 를 가지고 있거나 다른 reg 크기를 가지고 있는 시스템에는 적용하지 못하는 방법이라는 것이 주된 차이점이다.

![[Pasted image 20240907165222.png]]

- 위 그림은 4-way vertical layout 과 interleaved BP 의 차이를 보여주고 있는 것이다.
	- 일단 4-way 의 경우에는 SSE, AVX2, AVX512 에 대해 register size 가 2배씩 커짐에도 불구하고 성능 차이가 그리 크지 않은 것을 볼 수 있는데
	- Interleaved BP 의 경우에는 $2 \times$, $4 \times$ 씩 커지고 있는것을 볼 수 있다... 라고 하는데
	- 근데 그림을 잘못그린건지 그래프가 좀 이상하다.
		- 일단 그림을 그대로 해석하면 (세로축은 작을수록 좋기 때문에) SSE 가 제일 좋고, AVX2 는 SSE 보다 4배 안좋으며 AVX512 는 AVX2 보다 2배 좋은 것으로 보인다.
		- 즉, SSE > (2배) AVX512 > (2배) AVX2 이렇게 성능 차이가 있는 것으로 보이는데
		- 근데 본문에는 AVX512 > (2배) AVX2 > (2배) SSE 로 설명하고 있다.
- 추가적으로, BP 에 관한 선행 연구들에는 decompression speed 말고 다른 것에 집중한 것도 있다고 한다.
	- [BitWeaving](https://dl.acm.org/doi/10.1145/2463676.2465322) 에서는 HBP 와 VBP 라는 layout 을 제안하는데, 이들은 [[Single Instruction Multiple Data, SIMD (Arch)|SIMD]] 를 활용한 bit-parallelism 을 최대로 활용할 수 있게 해준다고 한다.
		- HBP 는 lookup (즉, exact match) 에 최적화되어있는 layout 이고
		- VBP 는 filtering 에 최적화되어있는 layout 이다.
	- [ByteSlice](https://dl.acm.org/doi/10.1145/2723372.2747642): 에서는 위의 BitWeaving 을 bit 단위가 아닌 byte 단위로 적용해 lookup 와 filtering 을 동시에 향상시키는 layout 이다.
	- 하지만 이들 둘 다 decompression 에 관한 것은 아니며, VBP 에 대한 decompression 방법을 제안한 [이 논문](https://dl.acm.org/doi/10.1145/2771937.2771943) 은 interleaved BP 보다 최대 30배는 더 느렸다고 한다.