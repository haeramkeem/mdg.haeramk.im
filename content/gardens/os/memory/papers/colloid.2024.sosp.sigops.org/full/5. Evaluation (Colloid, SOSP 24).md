---
tags:
  - os
  - os-memory
  - paper-review
date: 2025-01-27
title: "(논문) Tiered Memory Management: Access Latency is the Key!, SOSP 2024 (5. Evaluation)"
---
> [!info] 본 글은 논문 [Tiered Memory Management: Access Latency is the Key! (SOSP 2024)](https://dl.acm.org/doi/10.1145/3694715.3695968) 를 읽고 정리한 글입니다.

> [!info] 별도의 명시가 없는 한, 본 글의 모든 그림은 위 논문에서 가져왔습니다.

> [!info]- 목차
> - [[1. Introduction (Colloid, SOSP 24)|1. Introduction]]
> - [[2. Motivation (Colloid, SOSP 24)|2. Motivation]]
> - [[3. Colloid (Colloid, SOSP 24)|3. Colloid]]
> - [[4. Colloid with Existing Memory Tiering Systems (Colloid, SOSP 24)|4. Colloid with Existing Memory Tiering Systems]]
> - [[5. Evaluation (Colloid, SOSP 24)|5. Evaluation (현재 글)]]
> - [[6-7. Related Work and Conclusion (Colloid, SOSP 24)|6-7. Related Work and Conclusion]]

## 5.0. Setup

> [!tip] NXSECTION
> - `5.0` 은 evaluation setup 으로, 논문에는 이런 section 은 없다.

- 본 section 에서는 [[(논문) HeMem - Scalable Tiered Memory Management for Big Data Applications and Real NVM|HeMem]], [[(논문) MEMTIS - Efficient Memory Tiering with Dynamic Page Classification and Page Size Determination|MEMTIS]], [[(논문) TPP - Transparent Page Placement for CXL-Enabled Tiered-Memory|TPP]] 에 *Colloid* 를 적용했을 때와 안했을 때에 대한 evaluation 결과를 설명한다.
- 실험 setting 은:
	- 일단 [[2. Motivation (Colloid, SOSP 24)#2.1. Experimental Setup|Section 2.1.]] 에서와 동일한 환경이고
	- 설정값들은 별도 명시가 없는 한 전부 기본값으로 되어 있으며
	- TPP + Colloid 의 경우에는 [[(논문) Practical, Transparent Operating System Support for Superpages|Transparent Hugepages]] 를 활성화한 결과이고, 비활성화한 것은 [여기](https://github.com/host-architecture/colloid/tree/master) 에 있다고 한다 [^evaluation-thp-disable].
	- 그리고, $\epsilon = 0.01$ 으로, $\delta = 0.05$ 로 세팅하였다. 이때, 이 값을 변화시키며 실험한 것도 [여기](https://github.com/host-architecture/colloid/tree/master) 에 있다고 한다 [^evaluation-config-change].
- 실험에서 집중한 metric 은:
	- Steady-state 일때의 application throughput
	- Workload 혹은 memory interconnect contention 의 변화에 대한 convergence time
	- 그리고 real-world application 에 대해서는, 해당 application 에 specific 한 metric 이다.

## 5.1. Steady-state Throughput

### 5.1.0. Result

> [!tip] NXSECTION
> - `5.1.0` 은 실험 결과로, 논문에는 이런 section 은 없다.

- [[2. Motivation (Colloid, SOSP 24)#2.2.0. Result|Section 2.2.0.]] 와 동일한 그래프를 그려보면 다음과 같다.
- 일단 아래의 그림이 [[2. Motivation (Colloid, SOSP 24)#2.2.0. Result|Section 2.2.0.]] 의 그래프이고,

![[Pasted image 20250119190642.png]]

- 그리고 이 그림이 *Colloid* 를 추가했을 때의 그래프이다.

![[Pasted image 20250127221256.png]]

- 보면 일단 전체적으로 *Colloid* 를 활성화시켰을 때 성능이 대폭 향상하는 것을 알 수 있다.
- $0 \times$ 일 때는 모든 system 이 *Colloid* 가 있을 때와 없을 때에 동일한 성능을 보이고, best case 와도 유사한 성능을 보여주는 것을 알 수 있다.
- 근데 memory interconnect contention 이 증가함에 따라, benefit 이 *Colloid* 가 없을 때에 비해 훨씬 두드러지는 것을 볼 수 있다. 각각:
	- HeMem 은 $1.2 \times$ ~ $2.3 \times$,
	- TPP 는 $1.35 \times$ ~ $2.35 \times$,
	- MEMTIS 은 $1.29 \times$ ~ $2.3 \times$ 배의 throughput 이득이 있다.
- 또한 memory interconnect contention 과는 무관하게, *Colloid* 를 사용했을 경우 best case 와 유사한 성능을 보여준다. 각각 best case 와:
	- HeMem + Colloid 는 3%,
	- TPP + Colloid 는 8%,
	- MEMTIS + Colloid 는 13% 이내의 차이를 보여준다.

### 5.1.1. Understanding Colloid benefits.

- 그럼 [[2. Motivation (Colloid, SOSP 24)#2.2.1. Under memory interconnect contention, default tier access latency can exceed that of alternate tier.|Section 2.2.1.]] 의 그래프도 비교를 해보자. 아래놈이 [[2. Motivation (Colloid, SOSP 24)#2.2.1. Under memory interconnect contention, default tier access latency can exceed that of alternate tier.|Section 2.2.1.]] 의 그래프이고,

![[Pasted image 20250119192113.png]]

- 이놈은 *Colloid* 를 추가했을 때에 대한 그래프이다.

![[Pasted image 20250127223217.png]]

- [[2. Motivation (Colloid, SOSP 24)#2.2.1. Under memory interconnect contention, default tier access latency can exceed that of alternate tier.|Section 2.2.1.]] 에서와는 달리, 이제는 HeMem, TPP, MEMTIS 에서도 memory interconnect contention 이 증가함에 따라 alternate tier 에 더 많은 hot page 들을 위치시키는 것을 알 수 있다.
- 그리고 [[2. Motivation (Colloid, SOSP 24)#2.2.2. Existing systems continue to greedily place hottest pages in default tier under memory interconnect contention.|Section 2.2.2.]] 의 그래프도 비교를 해보자. 아래의 그래프가 [[2. Motivation (Colloid, SOSP 24)#2.2.2. Existing systems continue to greedily place hottest pages in default tier under memory interconnect contention.|Section 2.2.2.]] 의 것이고,

![[Pasted image 20250119192002.png]]

- 이놈은 *Colloid* 를 추가했을 때에 대한 그래프이다.

![[Pasted image 20250127223234.png]]

- 보면 일단 default tier 와 alternate tier 간의 latency 차이가 확연하게 줄어든 것을 볼 수 있다.
- $0 \times$ 일 때는 hot page 의 전부가 default tier 로 가있게 되는데, 이때에도 둘 간의 latency 차이를 줄이지 못해 여전히 default tier 의 latency 가 좀 더 낮은 것을 볼 수 있고,
- $1 \times$ 일 때는 default 와 alternate tier 간의 balance 가 맞은 것이며
- $2 \times$ 와 $3 \times$ 일 때는 alternate tier 에 모든 hot page 를 배치해도 여전히 latency 차이가 줄어들지 못해 alternate tier 의 latency 가 좀 더 낮은 것을 볼 수 있다.

### 5.1.2. Impact of alternate tier unloaded latency.

- 추가적으로 다양한 상황들에 대한 evaluation 을 진행해 다양한 상황에서 *Colloid* 가 주는 이점에 대해 실험했는데,
	- [[#5.1.2. Impact of alternate tier unloaded latency.|Section 5.1.2.]] 은 alternate tier 의 unloaded latency 에 변화를 주었을 때 *Colloid* 가 어떻게 반응하는지에 대한 실험이고,
	- [[#5.1.3. Impact of object size.|Section 5.1.3.]] 은 object size 를 (원래는 64byte 였는데) 바꿨을 때 *Colloid* 가 어떻게 반응하는지에 대한 실험이다.
	- 그리고 [여기](https://github.com/host-architecture/colloid/tree/master) 에 application 이 사용하는 core 개수를 변화시켰을 때 [^evaluation-application-core] 와 read-write ratio 를 변화시켰을 때 [^evaluation-rw-ratio] 의 실험 결과가 있다고 한다.
- 일단 [[Compute Express Link, CXL (Arch)|CXL]] 을 사용하게 되면, DRAM memory 보다 대략 latency 가 2배정도 차이난다고 한다. 그래서 지금까지의 실험에서는 default tier 와 alternate tier 간의 latency 가 1.9 배 차이나도록 하고 있었다고 한다.
- 근데 이 unloaded latency 를 바꾸기 위해, 이 실험에 대해서만 [[Uncore (Intel Arch)|uncore frequency]] 를 조정해 alternate tier 의 latency 를 1.9 배에서 2.7 배 까지 변화시킬 수 있었다고 한다.
	- 근데 이렇게 uncore frequency 를 건드는 것은 alternate tier 의 bandwidth 를 줄이는 side-effect 도 있다고 한다.
	- 따라서, 제시되는 값들은 "보수적인 수치" 라고 할 수 있다: 실제로는 해당 latency 를 가지는 alternate tier 의 bandwidth 는 이것보다 클 것이기 때문에, memory bandwidth saturation 에 의한 latency 의 증가가 이것보다는 더 적을 것이기 때문이다.
- 그래서 실험 결과는 다음과 같다:

![[Pasted image 20250127232705.png]]

- 일단 HeMem, TPP, MEMTIS 에 대한 실험 결과이고, 가로축은 unloaded latency 의 변화, 그리고 세로축은 memory interconnect contention intensity 의 변화이며 heatmap 의 각 cell 은 각 상황에 대한 throughput 증가 비율이다.
- 한눈에 볼 수 있는 것은 전반적으로 *Colloid* 를 사용했을 때가 더 좋다는 것이다.
- 구체적으로 두개의 관점에서 변화를 살펴보자.
	- 일단 동일한 unloaded latency 상에서는 (즉, 가로축은 고정하고 세로축의 변화만 보면) intensity 가 심해짐에 따라 *Colloid* 의 benefit 이 더 증가하는 것을 볼 수 있다 (위로 갈수록 색이 진해지니까).
		- 이것은 [[#5.1.0. Result|Section 5.1.0.]] 에서 본 것처럼 memory interconnect contention 이 심해질수록 hot page 를 alternate tier 에 두는 것이 더 좋기 때문이다.
	- 그리고 동일한 intensity 상에서는 (즉, 세로축은 고정하고 가로축의 변화만 보면) unloaded latency 가 증가함에 따라 *Colloid* 의 benefit 이 감소한다 (옆으로 갈수록 색이 옅어지니까).
		- 이것은 alternate tier 의 latency 가 워낙에 크기 때문에 default tier 의 latency 가 점점 증가해도 hot page 를 alternate tier 로 옮기기가 힘들기 때문이다.
		- 즉, alternate tier 의 latency 가 크기 때문에 평균 latency 가 더 높은 수준에서 형성되기 때문인 것.
		- 결과적으로  *Colloid* 의 "hot page 를 alternate tier 에 배치하는 것에서 오는 이점" 을 살리기 힘들어 이런 낮은 성능 향상을 보여주게 되는 것이다.
		- 그럼에도 불구하고, unloaded latency 가 가장 클 때에도, HeMem 은 $1.01 \times$ ~ $1.76 \times$ 의 성능 향상을, TPP 는 $1.03 \times$ ~ $1.76 \times$ 의 성능 향상을, MEMTIS $1.01 \times$ ~ $1.63 \times$ 의 성능 향상을 보여준다.

### 5.1.3. Impact of object size.

![[Pasted image 20250127234610.png]]

- 위 그림은 접근하는 object size 만을 64byte 에서 4096byte 로 단계적으로 바꿨을 때의 그래프이다.
	- 그래프의 각 축과 heatmap 이 의미하는 바는 가로축만 object size 를 64, 256, 1024, 4096 byte 로 바꾼 것이고 나머지는 [[#5.1.2. Impact of alternate tier unloaded latency.|Section 5.1.2.]] 와 동일하다.
- 일단 object size 가 256byte 보다 클 때, $0 \times$ intensity 에서도 *Colloid* 를 사용하는 것이 더 성능이 좋은 것을 확인할 수 있다:
	- HeMem 는 $1.17 \times$ ~ $1.31 \times$,
	- TPP 는 $1.18 \times$ ~ $1.35 \times$,
	- MEMTIS 는 $1.21 \times$ ~ $1.35 \times$ 배의 성능 향상이 있었다.
- 이것은 object size 가 커짐에 따라 workload 가 점점 더 memory intensive 해지기 때문이다.
	- 즉, 점점 더 memory intensive 해지기 때문에 $0 \times$ 에서도 memory interconnect contention 이 발생하여 *Colloid* 가 benefit 을 주게 된다는 것 [^object-size-sequential] [^object-size-l3miss].
	- 가령 4096byte 의 object size 를 사용하여 HeMem 실험을 하였더니 memory interconnect contention intensity 가 $0 \times$ 일 때에도 default tier 의 latency 가 alternate tier 의 latency 에 비해 1.77 배 더 컸다고 한다.
- 다만 intensity 가 클 때 object size 도 증가시키면 *Colloid* 의 benefit 이 점차 줄어드는 것으로 결과가 나왔다.
	- 이것은 alternate tier 의 BW 도 saturate 되기 때문이다.
	- 즉, default tier 뿐 아니라 alternate tier 의 latency 도 커져서 default tier 에 체류하는 hot page 의 수가 좀 더 많아지고 따라서 *Colloid* 에서의 "hot page 를 alternate tier 에 배치함으로써 오는 benefit" 이 적어지기 때문이다.
	- 가령 object size 가 64byte 일 때는 alternate tier 의 BW utilization 이 53% 정도였지만, 4096byte 일 때는 BW utilization 이 96% 까지 올라가게 된다고 한다.

### 5.1.4. Colloid CPU overheads.

- *Colloid* 에서는 latency measurement 와 page placement algorithm 때문에 추가적인 CPU cycle 을 필요로 한다.
- [[#5.1.0. Result|Section 5.1.0]] 에서는 이 CPU overhead 를 확인할 수 있는데, HeMem 와 MEMTIS 는 모두 2% 미만의 overhead 를 보여줬다고 한다.
	- 다만 TPP 에서는 4% ~ 6.5% 의 overhead 를 보여주었고, 이것은 [[4. Colloid with Existing Memory Tiering Systems (Colloid, SOSP 24)#4.3. TPP with Colloid|Section 4.3.]] 에서 말한 것 처럼 latency measurement 을 위해 추가적인 core 를 사용하기 때문이라고 한다.
	- 이것은 그래프에서는 $0 \times$ intensity 에서의 TPP 의 throughput 에서 보여진다고 생각된다; 이때 throughput 은 TPP 보다 TPP + Colloid 에서 조금 더 떨어지는데, 이게 위와 같은 이유에서인 것으로 보인다.

[^evaluation-thp-disable]: 확인해 보자.
[^evaluation-config-change]: 이것도 확인해 보자.
[^evaluation-application-core]: 이것도
[^evaluation-rw-ratio]: 그리고 이것도
[^object-size-sequential]: 이부분에 대해 본문에서는 access pattern 이 점점 더 sequential 으로 바뀌고, 따라서 HW prefetcher 가 더 효율적으로 작동하기 떄문이라고 한다. 그러나 이러한 변화는 Colloid 뿐 아니라 HeMem 이나 TPP 에 대해서도 동일하게 적용되는 것이라 성능 향상의 이유를 설명하기에는 어려워 보인다.
[^object-size-l3miss]: 추가적으로 이런 설명도 덧붙인다: CHA 와 L3 의 cache miss 의 비율이 per-core parallelism 의 척도가 되고, 이것은 object size 가 커짐에 따라 증가하기 때문에 object size 가 커짐에 따라 효율적이라는 것이다. 근데 cache miss 가 증가하는 것이 왜 효율적임을 방증하는 것인지 이해는 안된다.