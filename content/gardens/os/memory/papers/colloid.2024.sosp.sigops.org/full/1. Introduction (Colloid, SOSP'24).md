---
tags:
  - os
  - os-memory
  - paper-review
date: 2025-01-19
title: "(논문) Tiered Memory Management: Access Latency is the Key!, SOSP'24 (1. Introduction)"
---
> [!info] 본 글은 논문 [Tiered Memory Management: Access Latency is the Key! (SOSP 2024)](https://dl.acm.org/doi/10.1145/3694715.3695968) 를 읽고 정리한 글입니다.

> [!info] 별도의 명시가 없는 한, 본 글의 모든 그림은 위 논문에서 가져왔습니다.

> [!info]- 목차
> - [[1. Introduction (Colloid, SOSP'24)|1. Introduction (현재 글)]]
> - [[2. Motivation (Colloid, SOSP'24)|2. Motivation]]
> - [[3. Colloid (Colloid, SOSP'24)|3. Colloid]]
> - [[4. Colloid with Existing Memory Tiering Systems (Colloid, SOSP'24)|4. Colloid with Existing Memory Tiering Systems]]
> - [[5. Evaluation (Colloid, SOSP'24)|5. Evaluation]]
> - [[6-7. Related Work and Conclusion (Colloid, SOSP'24)|6-7. Related Work and Conclusion]]

> [!tip] NXSECTION
> - 이후에 등장하는 `1.x.` section 들은 논문 본문에는 없고 주인장이 임의로 자른것이다.

## 1.1. Tiered Memory

- Memory 의 중요성은 나날이 커지고 있다.
	- 가령 in-memory DB, graph processing engine, ML framework 들은 모두 이 memory 의 크기 혹은 대역폭에 영향을 받는다.
	- 그리고 cloud vendor 들에서 가장 큰 비용을 차지하는 것 또한 memory 이다.
- 하지만 [[Double Data Rate, DDR (Arch)|DDR]] 을 통해 CPU 에 노출되는 DRAM 은 확장성이 부족해 용량과 대역폭을 늘리기는 쉽지 않다.
	- "램슬롯" 이 부족해 더이상 메모리를 늘리지 못하는 상황을 생각하면 된다. 그만큼 이 DDR 은 확장성이 가난하다는 것.
	- 이 문제는 점점 심각해진다: CPU core 개수와 core 당의 concurrent memory request 의 개수는 기술이 발전하며 점점 커지기 때문에, 이 DDR 의 한계점이 bottleneck 이 된다.
	- 가령 최신의 Intel CPU 들은 memory 대역폭에 비해 몇배나 더 많은 memory traffic 을 만들어낼 수 있다고 한다 (Cascade Lake 는 2.98배, Ice Lake 는 3.31배, Sapphire Rapids 는 4.5배).
	- 심지어 AMD CPU 는 거의 10배에 가까운 traffic 을 생성해 낼 수 있다 (AMD Genoa 는 9.37 배의 traffic 을 쏟아낼 수 있을 정도로 빠르다).
- 따라서 이런 문제를 해결하기 위해 [[Tiered Memory Architecture (Arch)|Tiered memory architecture]] 가 등장하게 된다.
	- 이것은 이름이 시사하는 것처럼 [[Memory Hierarchy (Memory)|Memory hierarchy]] 에서 main memory 계층을 두개로 분리하여 "빠른 메모리" 와 "느린 메모리" 로 구성하는 것이다.
	- 구체적으로는 기존의 DDR 메모리가 하나의 tier 이고 DDR 이 아닌 방식으로 [[Direct Memory Access, DMA (OS)|cache coherent]] 한 메모리를 다른 방식으로 CPU 에서 접근 가능하게 하는 것이다.
	- 이 방식에서 요즘 각광을 받고 있는 것이 [[Compute Express Link, CXL (Arch)|CXL]] 이다: 이놈의 경우에는 기존의 메모리를 [[Peripheral Component Interconnect Express, PCIe (PCIe)|PCIe]] 로 장착해 CPU 에서 접근 가능하게 해준다.

## 1.2. Is Fast Memory Really "Fast"

- 이런 tiered memory architecture 에서는 DDR 에 추가적으로 장착한 메모리의 경우에는 대역폭은 높지만 latency 는 길기 때문에, latency 가 짧은 DDR 와 같이 사용할 때 "어떻게 메모리를 할당해 줄 것인가" 가 성능에 큰 영향을 미치게 된다.
	- 가령 자주 접근되는 hot page 의 경우에는 latency 가 짧은 쪽에 두는 것이 좋을거자나 그치?
- 그래서 이 "어떻게" 에 대한 연구가 많이 진행되어 왔는데, 이에 대한 최신의 SOTA 는 [[(논문) HeMem - Scalable Tiered Memory Management for Big Data Applications and Real NVM (SOSP'21)|HeMem]], [[(논문) MEMTIS - Efficient Memory Tiering with Dynamic Page Classification and Page Size Determination|MEMTIS]], 그리고 [[(논문) TPP - Transparent Page Placement for CXL-Enabled Tiered-Memory|TPP]] 이다.
- 이 연구들에서 공통적으로 가정하고 있는 것은 "hot page 는 default tier (즉, DDR 메모리) 에 두는 것이 가장 빠를 것이다" 이고, 따라서 hot page 를 최대한 많이 default tier 에 두려고 한다.
	- 즉, default tier 는 항상 alternative tier 에 비해 빠를 것 (latency 가 작을 것) 이라는 것.
- 하지만 본 논문에서는 이 가정을 깨는 것에서 부터 시작한다: memory request 가 동시다발적으로 많이 날라가는 경우에는, default tier 의 latency 는 alternative tier 에 비해 커질 수 있다는 것이다.
	- 심지어 bandwidth 가 아직 많이 남았음에도 이런 현상이 관측될 수 있다.
	- 이런 현상을 논문에서는 *Memory Interconnect Contention* 이라고 부른다. 즉, CPU-memory 사이의 datapath 에 대한 contention 이라는 것.
	- [[2. Motivation (Colloid, SOSP'24)|Section 2]] 에서 이에 대한 실험의 결과가 나오는데, 이러한 *Memory Interconnect Contention* 상황에서는 default tier 의 latency 는 기존의 5배까지 치솟을 수 있고, CXL 와 비교하면 이 latency 는 CXL 보다 2.5 배나 더 크다고 한다.
	- 따라서 기존의 연구들이 공통적으로 깔고 있던 가정이 깨지게 되었으므로, 이 상황 속에서는 기존의 연구들의 성능 또한 가난하게 나오게 된다: 구체적으로는, optimal 한 성능에 비해 2.3 배나 낮게 나온다고 한다.

## 1.3. Colloid

- 본 논문에서 제시한 system 의 이름은 *Colloid* 로, 이름처럼 default tier 와 alternative tier 가 잘 섞일 수 있게 해준다.
	- *Colloid* 에서는 "Balancing access latency" 라는 하나의 원칙에 따라 모든 memory placement 가 결정된다.
	- 즉, latency 가 작은 tier 에 memory page 를 배치하면 이 tier 의 latency 는 올라가고 반대로 기존에는 latency 가 높았던 tier 의 latency 는 작아지게 된다는 것.
	- 이것은 CPU 가 보낼 수 있는 memory request 의 양은 한정되어있기 때문이다.
	- 한정된 traffic 이 들어오는 상황에서, alternative tier 로 가는 traffic 을 default tier 로 보내게 된다면 default tier 에 더 많은 traffic 이 들어오기 때문에 latency 는 늘어나지만 alternative tier 에 들오는 traffic 은 줄어들어 이놈의 latency 도 줄어든다.
	- 결과적으로는 average latency 는 작아지게 되고, default tier 와 alternative tier 의 latency 가 같아지는 지점이 최소가 된다.
- 이 원칙 하나로 memory placement 를 조절하는 것만으로도 많은 것이 고려된다고 한다.
	- 일단 각 tier 의 기본 latency (unloaded latency) 가 자연스럽게 고려된다. 두 tier 가 모두 unloaded 인 상황에서는, default tier 가 latency 가 더 낮을 것이기 때문에 여기에 hot page 들이 배치된다.
	- 또한, 위에서 말한 *Memory Interconnect Contention* 상황 또한 커버가 된다. 간단하게 말하면 이 상황에서는 default tier latency 가 alternative tier latency 보다 클 수도, 작을 수도 있기 때문에, 이때에는 더 작은 latency 를 가지는 tier 에 배치하면 되는 것.
- 이러한 system 을 위해서는 두 가지가 필요할 것이다: (1) 어떻게 각 tier 의 access latency 를 측정할 것이냐. (2) 이렇게 측정한 latency 로 어떻게 memory placement 를 수행할 것이냐.
	- 따라서 *Colloid* 에서는 이 두가지에 대한 해결책을 제시한다.
	- 우선 (1) 을 위해 대부분의 최신 server 들에 탑재되는 HW 로 per-tier queue occupancy 와 request arrival rate 를 알아내고, *Little's law* (나중에 설명함) 에 따라 이것으로 access latency 를 계산하는 방법을 제시한다.
	- 그리고 (2) 를 위해 위에서 계산한 access latency 와 추가적인 per-page access tracking information 으로 memory page placement algorithm 을 제시한다.
- *Colloid* 는 지금까지 등장한 DDR, CXL 등과 같은 memory architecture 들과 잘 호환되고, 기존의 memory placement 기법 (가령 hugepage 등) 과도 잘 호환된다고 한다.
- 이를 보이기 위해 본 논문에서는 *Colloid* 를 HeMem, MEMTIS, TPP 와 결합하여 구현하고, 그에 대한 실험 결과를 공개한다.
- 또한 *Memory Interconnect Contention* 및 core count, object size 등과 같은 다양한 실험조건들을 변화시키며 실험을 진행하였고, 이러한 상황 속에서도 *Colloid* 가 near-optimal 한 성능을 보여준다는 것을 증명한다.
- 마지막으로, *Colloid* 의 반응속도에 대한 실험도 하여 workload 혹은 *Memory Interconnect Contention* 이 변화되었을 때도 민첩하게 그에 따라 대응하는 것 또한 증명한다.