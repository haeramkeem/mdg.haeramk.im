---
tags:
  - arch
  - originals
  - snu-aca25s
date: 2025-03-04
title: 1. Course Overview (Advanced Computer Architectures, SNU CSE)
---
> [!info] 서울대학교 컴퓨터공학부 유승주 교수님의 "고급 컴퓨터 구조" 강의를 필기한 내용입니다.
> - [[(SNU CSE) Advanced Computer Architectures|목차]]

## Scope

- 컴퓨터 구조에서의 핵심적인 rational 과 상용화된 [[Central Processing Unit, CPU (Arch)|CPU]] 및 [[Accelerator (Arch)|Accelerator]] 들의 구조에 대해 알아보고
- Recommend system 및 large model 두 가지의 workload 에 대한 HW 관점에서의 최적화에 대해 알아본다.

## 파일럿 에피소드

> [!warning] 문맥
> - 이 section 의 내용들은 좀 문맥이 엉망입니다: 그냥 쭉 훑어보며 어떤 내용들을 다루나 살펴보는 용도로만 여기 내용들을 남겨놓겠습니다.

### Workload 1: Recommender System

- 현시점 메타 혹은 구글에서 가장 중요한 캐시카우는 광고비이고, 이것은 "추천 시스템" 을 통해 구현된다.
	- 본 수업에서는 추천 시스템에 대해 알아보고, 메타의 [DLRM](https://github.com/facebookresearch/dlrm) 으로 실습을 해보며 실제 workload 가 어떻게 작동하고, 이를 통해 어떻게 아키텍쳐적인 관점에서 최적화를 할 수 있는지 알아본다.
- 컴퓨터 구조에서의 중요한 특성 중 하나는 데이터들이 반복해서 사용된다는 것이다.
	- 대부분의 컴퓨터 구조 철학이 이 가정을 기반으로 하고 있고, 따라서 만약에 데이터들을 한번밖에 사용하지 않는 상황이라면, 대부분의 최적화가 적용되지 못한다.
- 전통적으로는 데이터는 메모리에 있고, 연산은 CPU 에서 한다. 하지만 요즘은 [[Process In Memory, PIM (Arch)|PIM]] 와 같은 것들이 대두되며 데이터에 직접 가서 연산을 하는 ([[Near-Data Processing, NDP (Arch)|NDP]]) 추세이다.
	- 단순한 연산일지라도 이 방법을 사용하면 메모리 사용량을 크게 줄일 수 있는데, 왜냐면 average 와 같은 aggregation 연산을 하게 되면 필요한 데이터의 크기가 확 줄어들기 때문이다.
- Latency breakdown 을 통해 어떻게 최적화를 해야 할지 힌트를 얻을 수 있다.
	- 만약 latency 가 많이 걸리는 작업이 memory-bound job 이라면, PIM 을 사용하던지 아니면 메모리를 적게 사용하도록 알고리즘을 수정하는 것이 필요할 것이다.
	- 만약 그런 작업이 computation-heavy 한 작업이라면, 더 빠른 CPU 를 사용하거나 아니면 accelerator 등을 사용하면 될 것이다.
- 이전의 server 는 단순히 CPU 와 메모리만이 들어가는 형태였고, 추가적인 PCIe 포트로 GPU 등을 설치하는 형태였다면, 요즘은 CPU + 메모리로 구성된 단위인 blade 여러개와 accelerator module 이 탑재되는 *Zion architecture* 로 발전한다.
	- 이것은 2022년도까지 흥행하다 최근에는 NVIDIA 의 GraceHopper architecture 가 각광을 받고 있다.
	- 기존의 server 에서는 GPU 가 PCIe interface 를 통해 통신하고, 이로 인해 대역폭이 너무 작았었다면
	- GraceHopper 에서는 GPU 가 아주 큰 대역폭을 지원하는 NVLink 를 통해 interconnect 되어 있고, 그리고 memory 의 대역폭도 아주 커서 아주 뛰어난 성능을 보여준다고 한다.

### Workload 2: Large Language Model

- 본 강좌에서 학습하는 또 다른 workload 는 large model 이다. 이때에는 두가지의 세부 workload 로 분류할 수 있는데, 바로 training 와 inference 이다.
	- 이 둘의 차이점은 일단 training 의 경우에는 아주 많은 양의 데이터를 필요로 하고 아주 많은 양의 연산을 필요로 하지만, access pattern 은 이미 정해져 있다는 것이다.
	- 반면에 inference 의 경우에는 training 에 비해서는 적은 양의 데이터와 연산을 필요로 하지만, data access pattern 이 아주 dynamic 하다.
- 회사가 가지고 있는 자원은 한정되어 있기 때문에, training 과 inference 각각에 얼만큼의 자원을 할당해야 할지 정해야 한다.
	- 보통은 inference 에 더 많은 자원을 할당한다.
- Ops / byte 는 하나의 데이터가 몇번의 operation 에서 재사용되는지를 나타내는 단위이다. 즉, ops / byte 가 1이라면, 해당 데이터는 단 한번 사용되고 버려지는 셈이다.
	- 이전에는 이 값이 아주 컸기 때문에 cache 와 같은 것들을 사용했었다.
	- 하지만 LLM 에서 사용되는 parameter 는 이게 작기 때문에 다른 아키텍처가 필요하다.
- Model training 에 관해서는, 아주 많은 양의 데이터와 모델, 그리고 GPU 가 필요하다.
	- 본 강좌에서는 training 에 대한 실습도 진행하여 model training workload 의 특성에 대해서도 알아본다.
	- Training 은 몇달씩 걸리기도 하며, training 이 너무 오래 걸리면 training 을 하는 동안 새로운 모델이 나와 말짱 꽝이 될 수도 있다.
- 어떻게 데이터를 partitioning 하여 최대한 parallel 하게 데이터를 처리할지에 대해 알아야 한다.
	- 본 강좌에서는 어떤 parallelism 종류들이 있고,
	- Parallel execution 에 있어서 communication 해야 하는 데이터들은 어떤 것이 있으며,
	- 이러한 communication 과 computation 을 고려하여 어떤 parallel 방법론을 채택해야 하는지 배운다.
- Inference 와 관련해서는, 하나의 model 을 optimize 하는 것으로는 충분하지 않다; 그것을 사용하고 있는 solution 에 대해서도 같이 optimize 를 해야 한다.
	- 이를 위해 본 강좌에서는 효율성을 위한 small model 과 정확성을 위한 large model 을 섞어서 사용하는 Speculative decoding 와
	- Paged attention, flash attention 에 대해서도 배우고 이를 통해 inference workload 의 특성에 대해 배운다.
- 요즘 사용되는 floating point representation 은 [[IEEE 754 (Arch FP)|IEEE754]] 보다 낮은 precision 을 사용한다. 낮은 precision 을 통해 model 의 크기를 줄이면서도 비슷한 성능을 내는 방법에 대해서도 배운다.

### CPU Architecture

- CPU architecture 의 핵심적인 내용도 전반적으로 배우게 된다. 키워드만 짚어보면:
	- [[Dynamic Ramdom Access Memory, DRAM (Arch)|DRAM]] 의 구조와 이에 따른 최적화 방법들, 그리고 error correction 방법 ([[Error Correction Code, ECC (Arch)|ECC]])
	- [[Translation Lookaside Buffer, TLB (Memory)|TLB]], 와 multi level [[CPU Cache (Arch)|Cache]]
	- [[Interconnect (Arch)|Interconnect]]: 아무리 좋은 방법을 들고와도 interconnect 가 별로면 구린 성능이 나온다.
	- [[Out-of-Order Execution, OoO (Arch)|OoO]], [[Tomasulo's Algorithm (Arch)|Tomasulo]] 및 최신의 qualcomm mobile CPU architecture
	- [[Graphic Processing Unit, GPU (Arch)|GPU]] architecture, [[CUDA Execution Model (NVIDIA CUDA)|Thread]] 들의 latency hiding 방법
	- [[Non-Blocking Cache (CPU Cache)|Non-Blocking Cache]]:
		- Cache miss 를 처리하는 동안 cache 를 사용할 수 없다면 blocking 인 것이고
		- Miss 를 처리하는 동안에도 cache service 가 가능하게 하는 non-blocking cache 를 이용해 average cache miss panalty 를 낮출 수 있다.